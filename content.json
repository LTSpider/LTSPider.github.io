{"meta":{"title":"LTSpider Blogs","subtitle":null,"description":null,"author":"Henry Lee","url":"http://LTSpider.github.io"},"pages":[{"title":"categories","date":"2019-06-05T02:31:43.000Z","updated":"2019-06-11T21:59:09.758Z","comments":true,"path":"categories/index.html","permalink":"http://LTSpider.github.io/categories/index.html","excerpt":"","text":""},{"title":"about Author","date":"2019-06-05T07:06:00.000Z","updated":"2019-07-26T12:43:48.490Z","comments":true,"path":"about/index.html","permalink":"http://LTSpider.github.io/about/index.html","excerpt":"","text":"👱Brief Introduction​ Name : Henry ​ Sex : male ​ Work : IT Worker ​ School : Shaanxi institute of technology ​ Location : shanghai pudong new area ​ Hometown : Shaanxi xi’an ​ Description : welcome to my Blog . I’m Henry , I am a lonely person, I am silent, sharp eyes but very shallow. I desire to have a lot of friends but love alone and fear of the crowd, I sincerely desire to people and others to help. ​ 1、有时候你不得不承认，有一种感觉，它不能用文字描述，它不能用色彩渲染，它不能用味道表示，但却可 以轻易的被那叫做眼泪的东西淋漓尽致的传达出来！！！ ​ 2、“不要停止奔跑，不要回顾来路，来路无可眷恋，值得期待的只有前方。” ​ 3、 低头要有勇气，抬头要有底气。人生的冷暖取决于心灵的温度。理想和现实总是有差距的，幸好还有差 距，不然，谁还稀罕理想？ ​ 4、 生活不可能像你想象得那么好，但也不会像你想象得那么糟。我觉得人的脆弱和坚强都超乎自己的想象。 有时，我可能脆弱得一句话就泪流满面；有时，也发现自己咬着牙走了很长的路。 💻My Technology Stack​ Python： 自给自足状态。 ​ Linux : 踮脚尖触云端。 ​ docker： 可维生活酱子。 ​ JavaScript : 初生牛犊也怕虎。 🎵Favorite Music 💫Dream​ Still dreaming . ​ Buying a House ,Health , and Family ———&gt; Improving the Quality of Life ☎️Contact Email : HenryLee0718@163.com WeChat : Python_HenryLee QQ : 1172036591"},{"title":"tags","date":"2019-06-05T02:31:43.000Z","updated":"2019-06-11T21:56:38.825Z","comments":true,"path":"tags/index.html","permalink":"http://LTSpider.github.io/tags/index.html","excerpt":"","text":""},{"title":"正在更新中······","date":"2019-07-29T08:28:19.926Z","updated":"2019-07-29T08:28:19.926Z","comments":true,"path":"categories/database/index.html","permalink":"http://LTSpider.github.io/categories/database/index.html","excerpt":"","text":"增删改查(curd)curd的解释: 代表创建（Create）、更新（Update）、读取（Retrieve）和删除（Delete） 查询基本使用 查询所有列 select * from 表名; 例： select * from classes; 查询指定列 select id,name from classes; 定条件查询– 查询 name为小李飞刀的所有信息 select * from students where name=”小李飞刀”; – 查询 name为小李飞刀的所有信息 select * from students where id&gt;3; 可以使用as为列或表指定别名 select 字段[as 别名] , 字段[as 别名] from 数据表 where ….; 例: select name as 姓名,gender as 性别 from students; 字段的顺序 select id as 序号, gender as 性别, name as 姓名 from students; 增加 格式:INSERT [INTO] tb_name [(col_name,…)] {VALUES | VALUE} ({expr | DEFAULT},…),(…),… 说明：主键列是自动增长，但是在全列插入时需要占位，通常使用0或者 default 或者 null 来占位，插入成功后以实际数据为准 全列插入：值的顺序与表中字段的顺序对应 insert into 表名 values(…) 例： insert into students values(0,’郭靖‘,1,’蒙古’,’2016-1-2’); 部分列插入：值的顺序与给出的列顺序对应 insert into 表名(列1,…) values(值1,…) 例： insert into students(name,hometown,birthday) values(‘黄蓉’,’桃花岛’,’2016-3-2’); 上面的语句一次可以向表中插入一行数据，还可以一次性插入多行数据，这样可以减少与数据库的通信 全列多行插入：值的顺序与给出的列顺序对应 insert into 表名 values(…),(…)…; 例： insert into classes values(0,’python1’),(0,’python2’); insert into 表名(列1,…) values(值1,…),(值1,…)…; 例： insert into students(name) values(‘杨康’),(‘杨过’),(‘小龙女’); 修改 格式: UPDATE tbname SET col1={expr1|DEFAULT} [,col2={expr2|default}]…[where 条件判断] update 表名 set 列1=值1,列2=值2… where 条件 例： update students set gender=0,hometown=’北京’ where id=5; 删除 DELETE FROM tbname [where 条件判断] – 物理删除 – delete from 表名 where 条件 delete from students; – 整个数据表中的所有数据全部删除 delete from students where name=”小李飞刀”; ​ – 逻辑删除​ – 用一个字段来表示 这条信息是否已经不能再使用了​ – 给students表添加一个is_delete字段 bit 类型​ alter table students add is_delete bit default 0;​ update students set is_delete=1 where id=6;"},{"title":"正在更新中······","date":"2019-07-29T08:16:23.820Z","updated":"2019-07-29T08:16:23.820Z","comments":true,"path":"categories/backend/index.html","permalink":"http://LTSpider.github.io/categories/backend/index.html","excerpt":"","text":""},{"title":"正在更新中······","date":"2019-07-29T08:18:52.062Z","updated":"2019-07-29T08:18:52.062Z","comments":true,"path":"categories/frontend/index.html","permalink":"http://LTSpider.github.io/categories/frontend/index.html","excerpt":"","text":""},{"title":"正在更新中······","date":"2019-07-29T08:19:20.071Z","updated":"2019-07-29T08:19:20.071Z","comments":true,"path":"categories/ops/index.html","permalink":"http://LTSpider.github.io/categories/ops/index.html","excerpt":"","text":""},{"title":"北大一女博士说破股市：我用一只母鸡，就能把股市给你讲明白了","date":"2019-08-18T18:46:40.068Z","updated":"2019-08-18T18:46:40.068Z","comments":true,"path":"categories/tittle-tattle/index.html","permalink":"http://LTSpider.github.io/categories/tittle-tattle/index.html","excerpt":"","text":"一个故事看懂股市的本质： 从前，在一片广阔的大草原上，有很多只母鸡。 他们，吃着火锅唱着歌，吃着青草下着蛋。 但是，母鸡们又不愿意像这样日复一日的吃草，下蛋。 于是，一场革命性的变革即将到来…… 1：股票的产生 一天过去了…… 于是，猴子的50张母鸡券就被称为“股份”，而猴子的1000块钱投资，就叫“按20倍市盈率入股”。 啥叫市盈率？就是母鸡券的价格除以每年可以领到的鸡蛋，也可以理解为多久“回本”。 这个数字，越低越好。 母鸡关键词：股份；母鸡券（股票）；市盈率（市值÷每年净利润） 2：股东的权益 一年过去了…… 又过了一年…… 母鸡关键词：这就叫做有效率的再投资。 没过多久，聪明猴投资母鸡的事儿在动物圈传开了。 于是，动物圈的第一次股权交易发生了，而单张母鸡券的价格，也从20元，上浮到了22元。 这就是股价上浮的原因之一。 母鸡关键词：再投资；价值股（即高市盈率，高分红或者股息，也就是确定性收益比较高，但成长空间比较有限的公司）。 3：市场的形成 从此之后，动物圈算是炸窝啦~~各种动物都蠢蠢欲动，尤其是那些安安稳稳下蛋的母鸡们。 而母鸡必须在老虎家做体检，登记所有信息，方便动物们深入了解母鸡； 另一方面动物们也容易找到自己的交易对手，这样傻傻的犀牛大象也毫不费力就可以买卖母鸡券了！ 母鸡关键词：散户（一个悲伤的故事）。 4：股价 现在我们来做个大胆的猜想！ 那么在许许多多个夜晚，什么都没有发生，还是原来的母鸡，还是生普通的蛋，为什么母鸡券还是涨了跌了？ 原因很复杂，或许是猴子认为母鸡会变胖，或许是因为猩猩想娶媳妇儿急需用钱。 但无论什么原因，母鸡券的价格上涨或下降，母鸡都得不到任何直接的好处或坏处。 还是原来的母鸡，还是生普通的蛋。 而股价，有时候跟母鸡本身，并没有什么卵关系。 炒股不同于其他任何职业，付出和回报往往不成正比。其艰辛程度一般人未必承受的了，这也是为什么股票市场里面七亏二平一赢的原因。如果你是一名教师，多年后你可以桃李满天下，即使没有谋的高位，也受人尊敬。如果你是一名从政者，多年积累的人脉，足可以让你得到应有的回报。如果你是一名工程师，多年积累的技术经验，可以使你成为某一领域的专家。而炒股跟这些职业都不相同，是完全的结果论，即使你炒股五年、十年、二十年，如果你没有赚到钱，那么你的这些经验没有任何意义。 一、炒股是一条艰辛的道路，在通过财务自由之前，需要克服重重困难，想轻轻松松就能挣钱几乎是不可能的。 1、炒股很难得到家人的支持。即使处于今天思想高度开放的时代，大多数不炒股的人对炒股这件事情仍然会嗤之以鼻。现在正在进行炒股的散户们，有多少是瞒着老婆、爸妈进行的？有多少又是在巨额亏损后只能自舔伤口还要装作若无其事怕被家人发现的？有多少在连续莫名亏损后徘徊迷茫想放弃的？目前，整个中国社会对炒股存在严重的误解，对很多人来说炒股就是好逸恶劳，炒股就是买彩票，或者炒股就是变相赌博。在这种社会环境下，想成为职业股民需要承担多么大的心理负担。 2、炒股是需要“靠天吃饭”的。有人说炒股就是三年不开张，开张吃三年，虽然有些夸张，但却不无道理。2018年的行情下，任凭你技术再精通，有几人可以不亏损？又有几人可以赚钱？就如同农民种庄稼，好的收成需要老天成全。炒股也是一样，想要好的收益需要大盘的配合，也就是大的经济环境，包括国内经济和国际经济。如果真的是三年不开张，甚至亏损，有几人又能够撑过这三年呢？ 3、炒股没有一套标准化的盈利模式。炒股没有一个放之四海而皆准的标准公式，反而更是八仙过海各显神通。炒股的理论、流派、技术指标等都有很多种，再加上江湖上流传的各种炒股秘籍，更是花样百出，对于没有任何基础的股市小白来说找对努力的方向都很困难。现在又处在一个网络大数据时代，各种信息五花八门，更有很多以假乱真。关键炒股又是一件非常隐私的事情，很容易滥竽充数，我说去年炒股赚了20万，别人根本没办法去印证。 4、炒股是反人性的。多数人觉得炒股既简单又舒服，一人一桌一电脑，开着，喝着，按几下，白花花的银子就进兜里了。事实如何，只有真正以交易为生的人才知道。漫漫股海路上，充满诱惑和陷阱。庄家的一个空头陷阱很容易就把很多散户提出局外。当我们大赚时，我们激动不已，不断加仓，结果一个深度回调，获利尽吐，还被套牢。当我们被套牢后，我们愤怒不已，和盘托出，殊死一搏，结果被套的更深。其实，这些都是人类正常的情绪发泄，但是作为股票交易者不行，要时刻保持理性，不以涨喜，不以跌悲。 5、时刻都需要抉择。炒股更像是漂泊在茫茫大海上的一叶孤舟，没人会告诉你哪里才是彼岸。散户往往大多数时间都处于迷茫的状态，买进后跌，卖出后涨，空仓则踏空，全仓又套牢。市场当中每天充斥着各种各样的消息，而这些消息对于股市的影响到底有多大，谁也无法给出准确的答案。 二、股市又是相对公平的战场。 1、股市中没有人情往来。现代社会，有多少人厌倦工作中的虚情假意，人情往来。而炒股则不同，完全可以撕掉伪装的面具，一个人大门紧闭，一头钻进股市，按照自己的计划买卖。 2、股市中没有家事出身。对于有工作经验和生活阅历的人来说，这点深有体会。其实，很多人最开始投身股市的原因就在于此，工作中由于机遇或者家庭出身的原因，遇到难以突破的瓶颈，便寄希望于炒股进行一搏。 3、股市只有赚和亏。对于这点很多人可能不同意，认为股市中存在不少见不得人的勾当。虽然这点不可否认，但是试问各行各业哪一行又不是如此呢？能够随意操纵一直股票的涨跌的大资金毕竟不多，而除此之外十万、一百万甚至一千万对于股市来说有差别么？ 总之，炒股是一条非常艰辛的道路，一将功成万骨枯，如果你想实现财务自由，那么什么都不说努力吧，路漫漫其修远兮。值得庆幸的是，一旦你坚持下来成功 了，这一切又都是值得的，你会得到你想要的自由。 炒股，不割肉！这是我对大多数股民的建议。 在股市出现巨额的亏损，一般是两种情况：一种是追逐高位的热点股，被深度套牢；另一种是频繁交易，不断的亏损，不断的割肉，从而越亏越多。 我在股市里呆了有十多年，从我的经验来看，如果在低位的时候买入业绩不错的股票，就算市场大幅波动，其实亏损幅度也不会很大的，而且一旦市场企稳，这类股票一般会迅速收复失地。 很多时候，在股市出现巨亏，不是因为市场大幅下跌——毕竟像最近这样大幅下跌的行情还是很少见的——而是因为自己的不切实际的投资习惯造成的。 所以，你不要去追逐高位的热点股票，因为有可能会被深度套牢；也不要不停的交易，每一次买卖的决策都要谨慎再谨慎，不要轻易出手操作。 很多股民看到股市一涨就忍不住要往里冲，一两天不操作就手痒，一听说哪只股票有利好就兴奋，这种不良习惯不亏损才怪。 从另一个角度来看，股市是一个博弈的场所，你明白了大多数股民发生亏损的原因，跟他们反向操作，自然是能够赚钱的。 T+0解套技巧做T又分【加仓做T】和【减仓做T】 【加仓做T】 减仓做T也有2种方法，一种是”分时操作”，一种是”15分钟图操作”。 今天我们学习减仓做T”分时操作”减仓做T意思就是先卖出，后买入。(找卖点) 所谓的分时操作，就是我们在一只股票中看到的黄色分时均线。如下图 当价格跌破黄色分时均线的时候，选择卖出手中的一部分股票，比如是500股。然后等到低位的时候再接回来。 当价格反弹不能突破黄色分时均线的时候，也可以选择减仓做T，先卖出手中一部分股票。比如是500股。然后等到低位的时候再接回来。 同时，除了我们看这个黄色分时均线以外还有一个方法分析股票可以减仓做T。 行情跌破前期低点平台，减仓手中股票的一部分股数，然后当开始反弹的时候，及时买入接回相同的股数。这就是破前期低点平台的减仓做T方法。 最终我们总结而来，减仓做T无非就是要找到一个相对的卖点。怎么样才能分析出一个股票即将要下跌，只要能够分析出来就可以利用减仓做T降低成本。 那么至于什么时候可以接回来，大家可以重点学习解套第三部，加仓做T(找买点)。或者，行情下跌5个点了，你随时都可以接回来。因为你是赚钱的。 【减仓做T】 所谓的“满仓做T”就是现在有100万，我这100万已经满仓买了股票，手中没有子弹了，这个时候是很被动的，那么没有资金怎么能降低成本呢? 比如现在我这100万，买了2只股票各自占用50%仓位，也就是2只股票各自买了10万股，而这2只股票都是深套20%，那么应该怎样“满仓做T”呢? ”满仓做T”，就是我们在2只股票中先找到第一只股票拿来做T，首先在看盘软件上点击回车： 史上最受用的“T+0”解套方法全公开，满仓被套者看过来：教你如何正确做T+0！ 当我们找到该股的时候，发现股价跌破了黄色均价线，那么把持有10万股的股票卖出1万股，而这个1万股也就是你准备买入第二只被套的个股的资金。 第二步操作就是，接下来准备把刚才出来1万股买入第二只要解套的个股，从而降低成本，为何要降低第二只呢?解套要一只先来，第二只股要后来，先解套第二只才是重点，资金要互相利用起来! 史上最受用的“T+0”解套方法全公开，满仓被套者看过来：教你如何正确做T+0！ 当行情突破黄色均线之际，我们来把剩余的资金全部买入这只个股，等行情拉升5个点以上，或者多少，自己把握好这个度就行，那么现在仓位就是10万股+1万股=11万股，现在我们就利用之前出来的资金买入了该股，并且已经获利。 一线天筹码战法‘一线天筹码’抓牛股战法，相信关注我的粉丝都早已以验证了它的强大。 比如：盘龙药业、蓝晓科技、兆日科技、必创科技，这些成为一代妖王的股都具备这个技术特征，妖股基本上在启动第一板的时候，就会出现一线天筹码。所以，今天学会了这招，以后抓妖股一抓一个准。牛皮真的不是吹的，稍后我会对上面这几只妖股一一做分析，看完你就会觉得技术有多强大。 筹码一线天，顾名思义，一条顶格线的筹码峰，如下图：筹码突然出线一条顶格的直线峰，伴随着筹码成交量放大、换手也较大。 市场上一条线的筹码比较多，但具备以下几个特点的股最容易成妖，比如： 1、相对低位启动的一字涨停、跌停板或震幅较小涨跌停板（包括高开8个点以上涨停、一字涨停、T字涨停、一字板跌停、T字跌停）如下图： 第一种，高开9个多点开盘，接近一字板了。只要有一线天筹码也是合格的。 第二种：T字跌停板、或一字跌停都是算的。 第三种：T字涨停板，这种是最常见的。稍后重点分析。 第四种：一字板涨停。 2、筹码必须有一根顶格的直线，注意一定要是顶格的筹码线，周围可附带一些筹码，代表主力抢筹。 3、换手达到20%以上较理想。 4、涨停板量放大，比如T字板一盘是2倍以上的量，量能成倍放大的后作力强，当然也有一字板的一线天量能没有明显放大，但后期开板后也是有持续换手补量的过程。 5、除顶格筹码外，其他套牢筹码突然有大量消失的情况。如下图，前后两天筹码对比。 6、买入点： 第一种：高开4-5%以上，尽量不追，回落到一线天附近再进。 第二种：低于前一天涨停板，直接吃。 第三种，平开，直接吃 第四种，高开3-5%，激进者可以打底仓。 7、止盈点：其实没有一个固定的止盈点，这样的股很容易成妖，看能力，短线可以选择5%-20之间，中间可以等破趋势线止盈。 ８、止损点，满足下方任何一个条件都要止损： 一是破了20日均线；如下图，只要是破位了20日均线就出局。 二是破了趋势线，止损出局； 三是 三个交易日不突破一线天筹码线。看下图，确切的说应该是2个交易日不上攻就可以把它看成是弱势的一线天。 案例分析： 案例1：盘龙药业，3月22日出现一线天筹码，具备上面讲到量＋换手＋筹码，这天我在公众号股海睿道（gh600519）选出了这只股的，大家可以去看看，同一天还选出了朗新科技，都成功了。 案例2：兆日科技， 这股也是完全符合一线天筹码形态的，突破＋一线天筹码，这个一线天筹码旁边还围绕了一些小筹码，可以查看公众号文章，其实２０日就给了机会上车。 案例3：振华股份，３月２９日形成一线天形态，当天我们文章中还选出了广生堂，同样成功了。 案例4：凯发电气，跌停板的一线天，与涨停板的一线天有点不一样，下图5月22日，没有太大的变化，只觉得正常上涨。亮点在下一个交易日。 下图5月23日，一字跌停，盘中有破板，但是上方筹码少了很多，当天出现一根顶格筹码峰，重点是量能，量能成倍缩减，与涨停的相反，可见洗盘太明显。那么5月24日就是大买点。开盘买入后续至少可以挣20多个点。 案例５：蓝晓科技 案例６：中设股份 案例７：塞力斯 案例８：雷鸣科化，这个要重点讲一下，这个股回撤在２０％，也就是说出了一线天后还掉了２０％还上来了。 案例９：览海投资，这股我还买过。 （注意：导图看不清晰的可以找我要高清图片，这里会被压缩了）散户请牢记9张思维导图1、股市导图总纲 2、k线基础 3、均线基础 4、切线基础 5、指标分析 6、选股方法 7、板块轮动 8、统计分析 9、股市中的各色骗局 （注意：导图看不清晰的可以找我要高清图片，这里会被压缩了）剖析对手盘每一手交易的成交都伴随着买卖双方的同时诞生，也意味着卖方倾向认为价格将继续下跌，而买方倾向认为价格将继续上涨。换句话说，在一笔交易结束后，价格无论是下跌还是上升，成交的双方都会有其一判断错误。从这个层面来说，多空博弈就是一个零和游戏。主力资金在建仓阶段的主要目标就是用较少的资金去买入大量的便宜筹码。也就是说，作为主力资金的对手盘会认为投资标的的价格将会下跌，只有这样他们才愿意把大量的股票低价甩出去。对手盘有哪些类型?他们有什么交易特点?什么样的情况下对手盘才会低价卖出筹码呢?这就是主力资金每次交易操作计划前都会绞尽脑汁去思考的重要问题。 我们知道价格和时间成本是投资者考虑的两大重要因素。因此一是从资金获利和亏损的角度可以把原有的股票持有者分为两大类：套牢盘和获利盘;二是从持有时间的长短可以把原有的股票持有者分为三大类：短线、中线和长线投资者。这样如果从资金和时间两个变量去进行两两组合，就会六种类型的投资者。这里本书涉及到的是主力操盘的建仓阶段，因此大部分只讨论场内筹码的情况，至于场外资金的讨论留到后面的书本再做详细介绍。以下我们不妨来逐一对场内筹码持有者的交易特点进行分析。 (1)短线套牢盘：这类股票持有者一般在数日或数周内承受着亏损，他们更倾向于风险厌恶，对于短暂的价格波动非常敏感。只要亏损幅度不大(比如在3-10%以内)，他们都愿意继续持有股票，采取盯紧价格的观望态度。什么样的情况下短线套牢盘更愿意“低价”甩卖?我们可以从大盘和个股的波动状况进行思考。当大盘大跌而个股不跌甚至逆势上涨的时候，即使仍然亏损，但果断的短线套牢盘出于对未来个股补跌情况发生的担忧，更容易卖出筹码;当大盘大涨而个股不涨甚至下跌的情况下，一般来说由于转换标的成本不算太高，短线套牢盘也会较容易卖出股票选择较为强势的热门股。至于大盘和个股都不温不火的情况下，那么唯一能促使短线套牢盘有卖出冲动的恐怕就是考验他们的耐心了。在这类筹码卖出时所形成的压力一般都是较轻的，数量也不会太多。 (2)中线套牢盘：这类股票持有者一般在数月内承受着亏损，他们更倾向于保守型。别忘了，他们也是从短线套牢盘演变过来的。是什么造成了他们会“升级”成为中线套牢盘呢?一般都无外乎是不够果断及时止损，或者是价格早已跌破他们的心理防线(比如超过10%~20%)，“要是再给我一次机会，不赚钱我也一定卖出”或者“只要能回本或者亏一点点，我就认了卖了认倒霉算了”就是他们的典型心理。通常来说，单日大盘和个股的价格波动状况已经不再成为他们的关注点，他们唯一重视的就是他们的筹码成本价。(也正是因为这种趋向于麻木的心态，会使其逐渐步入长线深套的不归路。) 因此，让他们更容易“低价”抛售筹码的情况是这样的：一波强劲的上升行情一度逼近其成本价，让中线套牢盘心里燃起保本走人的希望，但是就偏偏在成本价面前“戛然而止”，无论怎样“上下徘徊”，还是“反复震荡”，甚至还有“掉头向下”的兆头，硬是不给他们全身而退的机会，在这种时候，他们的耐心会明显地比之前缺乏，而且更为浮躁，“算啦，老子玩不起还躲不起吗，认赔走人”就会成为他们离场的理由。要注意的是，由于中线套牢盘往往处于成交密集区，因此这类型的筹码数量较大，一旦蜂拥而出其卖压在短时间内可能是十分强大的。 (3)长线套牢盘：这类股票持有者一般在承受着一年甚至数年以上的亏损，麻木是他们最普遍的心态。经历过短线套牢和中线套牢,这类投资者期间应该还会抱有希望的加仓试图摊低成本,结果瘫在半山腰上,他们就是在不断的折腾中一而再再而三受伤的悲情角色。一般来说长线套牢盘的亏损幅度巨大，一般都超过50%以上。通常情况下，长线套牢盘是由于盲目追高而从牛市末期一直摔到漫漫熊市中去的，他们已经不再忍心去关注股市的一切，更不愿和别人谈论这种伤心事。“下辈子再也不炒股了”就是他们长线套牢盘内心的真实写照。什么样的情况下他们会甩卖出这种“血汗”筹码?就我的思考而言，大致有两种：一是因为极度恐慌而崩溃在熊市末尾的高潮中，二是在新牛市初期的财富效应带动下重新有了关注股市的冲动，这时“旧伤虽仍在，心痛却不再”。长线套牢盘在看到其他个股纷纷爆发起飞的同时回望自己仍趴在地下的“烂股票”，不由得心里会有了一种“壮士断背”的强烈欲望，而对其他热门的股票重新下注。 说白了，要使长线套牢盘以几乎“跳楼价”抛售出筹码，方法大致有两种：一是在熊市尾声阶段达到最高潮时，用更加凶狠恐怖的向下跳空大阴线甚至跌停的方式让所有悲观绝望的套牢盘交出剩下的股票;二是在新牛市初期以财富效应重新吸引其注意力后，通过用“时间换空间”的方式反复震荡，逐渐消磨其耐心，使其割肉并转向其他相对热门的个股。"}],"posts":[{"title":"运维工程师岗位面试题","slug":"运维工程师面试题","date":"2019-07-26T04:17:52.677Z","updated":"2019-08-01T07:02:10.845Z","comments":true,"path":"2019/07/26/运维工程师面试题/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/运维工程师面试题/","excerpt":"","text":"笔者其实没有想到去面试，只是在智联上更新了一下简历，就陆陆续续接到很多猎头的邮件和电话，实在是没准备好要去面试，就推掉了几家公司的面试了。正因为笔者也很久没有面试了，笔者也想去面试学习一下，闲话少说，下面就分享给大家笔者在2019年1月4号上午10点30分的面试经历： 首先，猎头或者公司人资会把公司的介绍及岗位要求发到你邮箱（或者QQ、微信），下面这份是猎头发给我的岗位说明，为了职业道德操守，公司的介绍和面试通知信息我就不贴出来了，我就把岗位要求贴出来： 职位描述： 1、 负责应用服务器的安装、配置、优化与维护； 2、 负责应用系统的日志信息备份、管理、维护与分析； 3、 负责应用系统的日常监测于维护、故障处理、性能分析与优化； 4、 负责应用部署系统、环境配置系统、监控系统的开发、部署、升级与维护，建设高性能的运维平台。 岗位要求: 1、 熟悉Linux操作系统的基础知识，熟练使用Linux常用操作命令； 2、 熟练配置Nginx、HAproxy 等应用相关软件的部署、配置与优化维护； 3、 熟悉网络基础知识、熟悉TCP/IP的工作原理，会配交换机或路由器，能熟练的对网络情况进行分析 4、 熟悉shell/perl/python中的一种或多种进行运维程序的开发； 5、 熟悉Nagios,Ganglia等监控软件 看着上面的要求大家是不是觉得要求也不高啊，你要细看就会发现，这家公司要求的还挺多，不仅要会网络知识（熟悉TCP/IP好像是每家单位的都会写这样的要求），还要会开发技能。相信很多做运维的兄弟在网络这一块是个头疼的事情，都对交换机和路由器不怎么会配置和管理。 然后，笔者详细了解他们公司，了解岗位要求，在突击复习一下可能会问到的知识点和技术点。到了面试的这天时间，早早的起床，把牙一定要刷干净，特别是有口臭的兄弟，最好准备点口香糖，到达面试公司前嚼块口香糖，以免因为口气的原因熏到面试官，让你在面试官心里减分。早点要记得吃，如果你是下午面试的话也要吃午饭，吃早点了精气神就有了。还要注意，带上你的简历和一支笔，虽然他们那边也会有你的简历，为了以防万一还是准备好简历。 ​ 最后，关键点来了，就是和面试官沟通了，有笔试的公司会让你做些面试题，没有笔试就直接和面试官聊了，下面是我和面试官沟通完之后记住的一些问题，分享给大家看一下，笔者一共记住了7个问题，好像还有两个问题实在想不起来了，如果大家有更恰当的回答一定要贴出来一起探讨和进步： 1、介绍下自己？（几乎每家公司首先都会让你做个自我介绍，好像是必修课一样） 笔者回答：此处省略笔者的自我介绍，笔者建议介绍自己的时间不宜过长，3-4分钟为宜，说多了面试官会觉得你太啰嗦了。说太少了也不行，那样会让人感觉你的经历太简单了、太空了。正常情况下，一般你在做自我介绍的同时，面试官这个时候在看你的简历，他需要一边看简历、一边听你介绍自己，如果你说个几句话就把自己介绍完了，他肯定还没缓过神来，对你的映像会减分的。在介绍的同时思维要清晰，逻辑要清楚，最好是根据你简历上写的经历来介绍，这样可以把面试官的思路带到你这里来，让他思路跟着你走。不要东扯一句，西扯一句。竟量少介绍自己的性格、爱好（最好能不说就不说），你可以简单罗列干过几家公司（最多罗列3家公司/也包含目前所在的公司，注意顺序不要乱），都在那几家公司负责什么工作，都用过什么技术，在着重介绍一下你目前所在的公司是负责哪些工作的，可以稍微详细一点介绍，不要让面试官听着晕头转向的感觉。 2、灰度发布如何实现？ 笔者回答：其实对这个问题笔者也答的不好，就不写出来误导大家了。大家有好的方法可以共享出来。不过笔事后在知呼上看到了一位网友的建议觉得不错，大家可以参考看一下 ：https://www.zhihu.com/question/20584476 3、Mongodb熟悉吗，一般部署几台？ 笔者回答：部署过，没有深入研究过，一般mongodb部署主从、或者mongodb分片集群；建议3台或5台服务器来部署。MongoDB分片的基本思想就是将集合切分成小块。这些块分散到若干片里面，每个片只负责总数据的一部分。 对于客户端来说，无需知道数据被拆分了，也无需知道服务端哪个分片对应哪些数据。数据在分片之前需要运行一个路由进程，进程名为mongos。这个路由器知道所有数据的存放位置，知道数据和片的对应关系。对客户端来说，它仅知道连接了一个普通的mongod，在请求数据的过程中，通过路由器上的数据和片的对应关系，路由到目标数据所在的片上，如果请求有了回应，路由器将其收集起来回送给客户端。 4、如何发布和回滚，用jenkins又是怎么实现？ 笔者回答：发布：jenkins配置好代码路径（SVN或GIT），然后拉代码，打tag。需要编译就编译，编译之后推送到发布服务器（jenkins里面可以调脚本），然后从分发服务器往下分发到业务服务器上。 回滚：按照版本号到发布服务器找到对应的版本推送 5、Tomcat工作模式？ 笔者回答：Tomcat是一个JSP/Servlet容器。其作为Servlet容器，有三种工作模式：独立的Servlet容器、进程内的Servlet容器和进程外的Servlet容器。 进入Tomcat的请求可以根据Tomcat的工作模式分为如下两类： Tomcat作为应用程序服务器：请求来自于前端的web服务器，这可能是Apache, IIS, Nginx等； Tomcat作为独立服务器：请求来自于web浏览器； 6、监控用什么实现的？ 笔者回答：现在公司的业务都跑在阿里云上，我们首选的监控就是用阿里云监控，阿里云监控自带了ECS、RDS等服务的监控模板，可结合自定义报警规则来触发监控项。上家公司的业务是托管在IDC，用的是zabbix监控方案，zabbix图形界面丰富，也自带很多监控模板，特别是多个分区、多个网卡等自动发现并进行监控做得非常不错，不过需要在每台客户机（被监控端）安装zabbix agent。 7、你是怎么备份数据的，包括数据库备份？ 笔者回答：在生产环境下，不管是应用数据、还是数据库数据首先在部署的时候就会有主从架构、或者集群，这本身就是属于数据的热备份；其实考虑冷备份，用专门一台服务器做为备份服务器，比如可以用rsync+inotify配合计划任务来实现数据的冷备份，如果是发版的包备份，正常情况下有台发布服务器，每次发版都会保存好发版的包。 总结一下面试注意几点事项，可能笔者也说得不太对，为了我们运维工作的兄弟们都能拿到高薪，大家一定要指证出来一起进步、一起探讨： 第一，你要对自己的简历很熟悉，简历上的写的技能自己一定要能说出个一二，因为面试官的很多问题都会挑你简历上写的问。比如你简历上写了这么一条技能“熟悉mysql数据库的部署安装及原理”。你即然写了这么一条技能，你在怎么不熟悉你也要了解mysql的原理，能说出个大概意思。万一面试官问到了你写的这一条，你都答不上来，那在他心里你又减分了，基本上这次面试希望不大。 第二，如果面试官问到你不会的问题，你就说这个不太熟悉，没有具体研究过，千万别不懂装懂，还扯一堆没用的话题来掩饰，这样只会让面试官反感你。 第三，准备充分，竟可能多的记住原理性的知识，一般面试问的多的就是原理。很少问具体的配置文件是怎么配置的。面试前也要了解清楚“职位描述”和“岗位要求”，虽然有时候大多数不会问到岗位要求的问题，但也要了解和熟悉。 第四，面试完后一定要总结，尽量记住面试官问的每一个问题，回去记录下来，如果问到不会的问题，事后要立马查百度或者找朋友搞清楚、弄明白，这样你才能记劳，下次面试说不定又问到同样的问题。 问完之后，面试官就跟我聊薪资待遇了，问我多少钱能达到自己的要求，我就不便透露了，可以私聊，哈哈，后续笔者会陆陆续续更新以前面试的经历和问题，有需要的朋友可以转载或者收藏起来一起讨论。 岗位职责：1、负责公司产品的版本控制、构建和发布管理；2、负责公司统一配置库管理工作，权限管理与分配准确及时，定期完成配置备份；3、负责公司内部开发/测试服务器的运行管理工作；4、负责Linux操作系统的安装、配置、监控和维护、问题处理、软件升级、 数据备份、应急响应、故障排除等、保证线上环境的稳定运行；5、负责支撑平台24×7稳定运行，并进行前瞻性容量规划；6、负责公司机房服务器日常维护及网络系统安装、部署、维护工作。 岗位要求：1、计算机相关专业本科及以上学历，2年以上运维或配置管理工作经验；2、至少熟悉一种监控系统搭建，如Nagios/Zabbix/等；3、至少熟悉一种集群管理工具，如Ansible/SaltStack等；4、有使用集成发布工具发布构建经验优先。比如：bamboo或者Jenkins；5、熟悉Unix/Linux操作系统，熟悉Weblogic/tomcat等中间件，能够编写shell脚本，熟悉软件开发过程及过程产品，有一定的网络基础；6、熟悉rsyslog, flume等日志收集和处理系统；7、具有强烈的安全意识及较强的沟通协调和学习能力，良好的团队合作精神，工作积极主动。 过去之后，前台美眉把我带到他们公司的地下室，我扫视了一下周围的环境，貌似旁边就是机房，因为我听到服务器的声音。等了几分钟，面试官下来了，面试官目测比较瘦，看着跟我身材差不多（应该不到120），他说他是负责运维部的，然后开始就叫我先自我介绍，都是一个套路，免不了介绍的，所以兄弟们一定要把自我介绍练好。然后开始问我问题了，跟面试官聊得还行，问我应该有不下10个以上的问题，我记住了下面有10个问题： 1、LVS负载的原理，和Nginx负载有啥区别？ 笔者回答：这个问题我觉得面试官司没问好，正常都会这么问“LVS有哪些负载均衡技术和调度算法?”。我回答就是按我说的这种问法回答的，反正他也频繁点头，当然，笔者回答的可能没有下面我整理出来的那么详细，大概意思我都说明白了。 LVS是Liunx虚拟服务器的简称，利用LVS提供的负载均衡技术和linux操作系统可实现高性能、高可用的服务器集群，一般LVS都是位于整个集群系统的最前端，由一台或者多台负载调度器（Director Server）组成，分发给应用服务器（Real Server）。它是工作在4层（也就是TCP/IP中的传输层），LVS是基于IP负载均衡技术的IPVS模块来实现的，IPVS实现负载均衡机制有三种，分别是NAT、TUN和DR，详述如下：  VS/NAT： 即（Virtual Server via Network Address Translation） 也就是网络地址翻译技术实现虚拟服务器，当用户请求到达调度器时，调度器将请求报文的目标地址（即虚拟IP地址）改写成选定的Real Server地址，同时报文的目标端口也改成选定的Real Server的相应端口，最后将报文请求发送到选定的Real Server。在服务器端得到数据后，Real Server返回数据给用户时，需要再次经过负载调度器将报文的源地址和源端口改成虚拟IP地址和相应端口，然后把数据发送给用户，完成整个负载调度过程。 可以看出，在NAT方式下，用户请求和响应报文都必须经过Director Server地址重写，当用户请求越来越多时，调度器的处理能力将称为瓶颈。  VS/TUN ：即（Virtual Server via IP Tunneling） 也就是IP隧道技术实现虚拟服务器。它的连接调度和管理与VS/NAT方式一样，只是它的报文转发方法不同，VS/TUN方式中，调度器采用IP隧道技术将用户请求转发到某个Real Server，而这个Real Server将直接响应用户的请求，不再经过前端调度器，此外，对Real Server的地域位置没有要求，可以和Director Server位于同一个网段，也可以是独立的一个网络。因此，在TUN方式中，调度器将只处理用户的报文请求，集群系统的吞吐量大大提高。  VS/DR： 即（Virtual Server via Direct Routing） 也就是用直接路由技术实现虚拟服务器。它的连接调度和管理与VS/NAT和VS/TUN中的一样，但它的报文转发方法又有不同，VS/DR通过改写请求报文的MAC地址，将请求发送到Real Server，而Real Server将响应直接返回给客户，免去了VS/TUN中的IP隧道开销。这种方式是三种负载调度机制中性能最高最好的，但是必须要求Director Server与Real Server都有一块网卡连在同一物理网段上。 回答负载调度算法，IPVS实现在八种负载调度算法，我们常用的有四种调度算法（轮叫调度、加权轮叫调度、最少链接调度、加权最少链接调度）。一般说了这四种就够了，也不会需要你详细解释这四种算法的。你只要把上面3种负载均衡技术讲明白面试官就对这道问题很满意了。接下来你在简单说下与nginx的区别： LVS的优点： 抗负载能力强、工作在第4层仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的；无流量，同时保证了均衡器IO的性能不会受到大流量的影响；工作稳定，自身有完整的双机热备方案，如LVS+Keepalived和LVS+Heartbeat；应用范围比较广，可以对所有应用做负载均衡；配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率。LVS的缺点： 软件本身不支持正则处理，不能做动静分离，这就凸显了Nginx/HAProxy+Keepalived的优势。如果网站应用比较庞大，LVS/DR+Keepalived就比较复杂了，特别是后面有Windows Server应用的机器，实施及配置还有维护过程就比较麻烦，相对而言，Nginx/HAProxy+Keepalived就简单一点Nginx的优点： 工作在OSI第7层，可以针对http应用做一些分流的策略。比如针对域名、目录结构。它的正则比HAProxy更为强大和灵活；Nginx对网络的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势所在；Nginx安装和配置比较简单，测试起来比较方便；可以承担高的负载压力且稳定，一般能支撑超过几万次的并发量；Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点；Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。LNMP现在也是非常流行的web环境，大有和LAMP环境分庭抗礼之势，Nginx在处理静态页面、特别是抗高并发方面相对apache有优势；Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，有需求的朋友可以考虑用其作为反向代理加速器；Nginx的缺点： Nginx不支持url来检测。Nginx仅能支持http和Email，这个它的弱势。Nginx的Session的保持，Cookie的引导能力相对欠缺。 2、redis集群的原理，redis分片是怎么实现的，你们公司redis用在了哪些环境？ 笔者回答：reids集群原理： 其实它的原理不是三两句话能说明白的，redis 3.0版本之前是不支持集群的，官方推荐最大的节点数量为1000，至少需要3(Master)+3(Slave)才能建立集群，是无中心的分布式存储架构，可以在多个节点之间进行数据共享，解决了Redis高可用、可扩展等问题。集群可以将数据自动切分(split)到多个节点，当集群中的某一个节点故障时，redis还可以继续处理客户端的请求。 redis分片： 分片(partitioning)就是将你的数据拆分到多个 Redis 实例的过程，这样每个实例将只包含所有键的子集。当数据量大的时候,把数据分散存入多个数据库中,减少单节点的连接压力,实现海量数据存储。分片部署方式一般分为以下三种： （1）在客户端做分片；这种方式在客户端确定要连接的redis实例，然后直接访问相应的redis实例； （2）在代理中做分片；这种方式中，客户端并不直接访问redis实例，它也不知道自己要访问的具体是哪个redis实例，而是由代理转发请求和结果；其工作过程为：客户端先将请求发送给代理，代理通过分片算法确定要访问的是哪个redis实例，然后将请求发送给相应的redis实例，redis实例将结果返回给代理，代理最后将结果返回给客户端。 （3）在redis服务器端做分片；这种方式被称为“查询路由”，在这种方式中客户端随机选择一个redis实例发送请求，如果所请求的内容不再当前redis实例中它会负责将请求转交给正确的redis实例，也有的实现中，redis实例不会转发请求，而是将正确redis的信息发给客户端，由客户端再去向正确的redis实例发送请求。 redis用在了哪些环境： java、php环境用到了redis，主要缓存有登录用户信息数据、设备详情数据、会员签到数据等 3、你会怎么统计当前访问的IP，并排序？ 笔者回答：统计用户的访问IP，用awk结合uniq、sort过滤access.log日志就能统计并排序好。一般这么回答就够了，当然你还可以说出其它方式来统计，这都是你的加分项。 4、你会使用哪些虚拟化技术？ 笔者回答：vmware vsphere及kvm，我用得比较多的是vmware vsphere虚拟化，几本上生产环境都用的vmware vsphere，kvm我是用在测试环境中使用。vmware 是属于原生架构虚拟化技术，也就是可直接在硬件上运行。kvm属于寄居架构的虚拟化技术，它是依托在系统之上运行。vmware vcenter 管理上比较方便，图形管理界面功能很强大，稳定性强，一般比较适合企业使用。KVM管理界面稍差点，需要管理人员花费点时间学习它的维护管理技术。 5、假如有人反应，调取后端接口时特别慢，你会如何排查？ 笔者回答：其实这种问题都没有具体答案，只是看你回答的内容与面试官契合度有多高，能不能说到他想要的点上，主要是看你排查问题的思路。我是这么说的：问清楚反应的人哪个服务应用或者页面调取哪个接口慢，叫他把页面或相关的URL发给你，首先，最直观的分析就是用浏览器按F12，看下是哪一块的内容过慢（DNS解析、网络加载、大图片、还是某个文件内容等），如果有，就对症下药去解决（图片慢就优化图片、网络慢就查看内网情况等）。其次，看后端服务的日志，其实大多数的问题看相关日志是最有效分析，最好用tail -f 跟踪一下日志，当然你也要点击测试来访问接口日志才会打出来。最后，排除sql，，找到sql去mysql执行一下，看看时间是否很久，如果很久，就要优化SQL问题了，expain一下SQL看看索引情况啥的，针对性优化。数据量太大的能分表就分表，能分库就分库。如果SQL没啥问题，那可能就是写的逻辑代码的问题了，一行行审代码，找到耗时的地方改造，优化逻辑。 6、mysql数据库用的是主从读写分离，主库写，从库读，假如从库无法读取了、或者从库读取特别慢，你会如何解决？ 笔者回答：这个问题笔者觉得回答的不太好，对mysql比较在行的朋友希望能给点建议。以解决问题为前提条件，先添加从库数量，临时把问题给解决，然后抓取slow log ，分析sql语句，该优化就优化处理。慢要不就是硬件跟不上，需要升级；要不就是软件需要调试优化，等问题解决在细化。 7、cpu单核和多核有啥区别？ 笔者回答：很少有面试官会问这样的问题，即然问到了，也要老实回答。还好笔者之前了解过CPU，我是这么说的：双核CPU就是能处理多份任务，顺序排成队列来处理。单核CPU一次处理一份任务，轮流处理每个程序任务。双核的优势不是频率，而是对付同时处理多件事情。单核同时只能干一件事，比如你同时在后台BT下载，前台一边看电影一边拷贝文件一边QQ。 8、机械磁盘和固态硬盘有啥区别？ 笔者回答：我擦，啥年代了，还问磁盘的问题，这面试官有点逗啊。那也要回答啊： HDD代表机械硬盘，SSD代表固态硬盘。首先，从性能方面来说，固态硬盘几乎完胜机械硬盘，固态硬盘的读写速度肯定要快机械硬盘，因为固态硬盘和机械硬盘的构造是完全不同的（具体的构造就没必要解释了）。其次，固态盘几乎没有噪音、而机械盘噪音比较大。还有就是，以目前的市场情况来看，一般机械盘容量大，价格低；固态盘容量小，价格偏高。但是企业还是首选固态盘。 9、说一下用过哪些监控系统？ 笔者回答：这个监控的问题又问到了，笔者在2019年1月4号也被问到类似这样的问题，笔者曾经用过zabbix、nagios、 cacit等。但是在这次面试中只说用过zabbix和nagios。说完了之后，面试官就让我说一下这两个监控有啥区别： 从web功能及画图来讲： Nagios简单直观，报警与数据都在同一页面， 红色即为问题项。Nagios web端不要做任何配置。 Nagios需要额外安装插件，且插件画图不够美观。 Zabbix监控数据与报警是分开的，查看问题项需要看触发器，查看数据在最新数据查看。而且zabbix有很多其它配置项， zabbix携带画图功能，且能手动把多个监控项集在一个图中展示。 从监控服务来讲： Nagios自带的监控项很少。对一些变动的如多个分区、多个网卡进行监控时需要手动配置。 Zabbix自带了很多监控内容，感觉zabbix一开始就为你做了很多事，特别是对多个分区、多个网卡等自动发现并进行监控时，那一瞬间很惊喜，很省心的感觉。 从批量配置和报警来讲： Nagios对于批量监控主机，需要用脚本在server端新增host，并拷贝service文件。 Nagios用脚本来修改所有主机的services文件，加入新增服务。 Zabbix在server端配置自动注册规则，配置好规则后，后续新增client端不需要对server端进行操作。 Zabbix只需手动在模板中新增一监控项即可。 总体来讲： Nagios要花很多时间写插件，Zabbix要花很多时间探索功能。 Nagios更易上手，Nagios两天弄会，Zabbix两周弄会。 Zabbix画图功能比Nagios更强大 Zabbix对于批量监控与服务更改，操作更简洁；Nagios如果写好自动化脚本后，也很简单，问题在于写自动化脚本很费神。 10、给你一套环境，你会如何设计高可用、高并发的架构？ 笔者回答： 如果这套环境是部署在云端(比如阿里云)，你就不用去考虑硬件设计的问题。可直接上阿里云的SLB+ECS+RDS这套标准的高可用、高并发的架构。对外服务直接上SLB负载均衡技术，由阿里的SLB分发到后端的ECS主机；ECS主机部署多台，应用拆分在不同的ECS主机上，尽量细分服务。数据库用RDS高可用版本（一主一备的经典高可用架构）、或者用RDS金融版（一主两备的三节点架构）。在结合阿里其它的服务就完全OK，业务量上来了，主机不够用了，直横向扩容ECS主机搞定。 如果这套环境托管在IDC，那么你就要从硬件、软件（应用服务）双面去考虑了。硬件要达到高可用、高并发公司必须买多套网络硬件设备（比如负载设备F5、防火墙、核心层交换、接入层交换）都必须要冗余，由其是在网络设计上，设备之间都必须有双线连接。设备如果都是跑的单机，其中一个设备挂了，你整个网络都瘫痪了，就谈不上高可用、高并发了。其次在是考虑应用服务了，对外服务我会采用成熟的开源方案LVS+Keepalived或者Nginx+Keepalived，缓存层可以考虑redis集群及Mongodb集群，中间件等其它服务可以用kafka、zookeeper，图片存储可以用fastDFS或MFS，如果数据量大、又非常多，那么可采用hadoop这一套方案。后端数据库可采用 “主从+MHA”。这样一套环境下来是绝对满足高可用、高并发的架构。 岗位职责：1、日常线上项目的需求处理；2、新项目上线对接的相关工作；3、日常运维工具开发、维护、优化；4、监控业务的运行状态，及时处理项目运行中出现的故障，保障项目服务24x7稳定运行；5、分析排除系统、数据库、网络、应用等故障及错误；6、负责服务器的资源调配和系统安全、数据备份。任职要求： 熟悉linux操作系统， 熟练使用一种或多种脚本语 言（例如 Python/Perl/Shell）； 熟悉至少一种共有云技术，多种运维平台工具（Nagios, Zabbix，Puppet等） 熟悉Nginx,Mysql, Redis, Keepalived, LVS等中间件的配置与调优； 熟悉网络部署，多种数据机房故障的发现和排除的工具，有做个跨机房数据同步的优先； 熟悉mysql、redis、mongoDB的安装、维护、性能优化； 了解反向代理、负载均衡原理. 有责任心，耐心，积极肯学的心态以及良好的沟通表达能力和团队合作精神； 其实这个要求，我在上一篇文章也说到过，大多数公司都写得差不多，很多公司自己懒的写，直接照搬别的公司发出来岗位要求，所以我们只要了解它就可以了，面试的时候不一定会问到这些岗位的要求说明，你看这家公司没有写熟悉TCP/IP，其实面试官这一次有问到TCP/IP这个问题的。这次技术面试后总体面试官还是比较满意，后来猎头通知我一面过了，准备安排2018年1月11号下午进行二面（跟我谈薪资、对海外工作的想法、人生规划等话题）。好了，不多说了，大家自己慢慢看我和面试官聊的技术问题吧。 1、介绍下自己？ 笔者回答：不管是电话面试还是现场面试，自我介绍是避免不了的，上一篇文章我有详细介绍这块的内容，这里不做解释了，感兴趣的朋友参考我上一篇文章： 总结一下：运维工程师面试的经历及面试相关问题（会持续更新） 2、为什么想着要离开现在的公司？ 笔者回答：虽然是面试技术，但也会有很多面试官会不经意的问你这个问题，看起来很随意的问题，其实这个问题里面隐藏了很多信息，最直观的就是看你这个人对企业的忠诚度、还能看你是不是心浮气燥的性格等等。如果你曾经频繁跳过槽，不管出于什么原因，笔者个人都不建议写在简历上，最好能够合并一些工作时间和单位，企业是很担心把你招来后会不会短时间你又跳槽了。当然如果都是因为企业经营不善倒闭所至，就没关系了。说到这里，就想起了笔者曾经一位同事，连续在好几家单位都干倒闭了，这我也不知道说啥好了。。。好了，咋们接着往下走。 3、TCP/IP原理说一下？TCP有哪几个状态，分别是什么意思？ 笔者回答：以tcp/ip协议为核心,分五层。tcp工作在第4层，主要有tcp和udp协议。其中tcp是可靠协议，udp是不可靠协议。 tcp传输之前，需要建立连接，通过三次握手实现。 TCP三次握手状态：首先是closed状态，当发起连接后，进入Listen状态，当三次握手之后，进入EST状态。三次握手中间还有一个临时状态:SYN_SENT。SYN_SENT 当应用程序发送ack之后，进入EST状态,如果没有发送，就关闭closed. 总结：大家一定要熟记tcp状态转换图，参考 http://blog.csdn.net/wenqian1991/article/details/40110703 4、有个客户说访问不到你们的网站，但是你们自己测试内网和外网访问都没问题。你会怎么排查并解决客户的问题？ 笔者回答：我们自己测了都没问题，只是这个客户访问有问题，那肯定是要先联系到这个客户，能远程最好，问一下客户的网络是不是正常的，访问其它的网站有没有问题（比如京东、百度什么的）。如果访问其它网站有问题，那叫客户解决本身网络问题。如果访问其它网站都没问题，用ping和nslookup解析一下我们的网站是不是正常的，让客户用IP来访问我们的网站是否可行，如果IP访问没问题，那就是客户的DNS服务器有问题或者DNS服务器解析不到我们的网站。还有一种可能就是跨运营商访问的问题，比如我们的服务器用的是北方联通、而客户用的是南方移动，就也有可能突然在某个时间段访问不到，这种情况在庞大的中国网络环境中经常发生（一般是靠CDN解决）。还有可能就是我们的网站没有SSL证书，在公网是使用的是http协议，这种情况有可能就是没有用https协议网站被运营商劫持了。 5、redhat 6.X版本系统 和 centos 7.X版本有啥区别？ 笔者回答：桌面系统（6/GNOE2.x、7/GNOME3.x）、文件系统（6/ext4、7/xfs）、内核版本（6/2.6x、7/3.10x）、防火墙（6/iptables、7/firewalld）、默认数据库（6/mysql、7/mariadb）、启动服务（6/service启动、7/systemctl启动）、网卡（6/eth0、7/ens192）等。 6、你会用什么方法查看某个应用服务的流量使用情况？ 笔者回答：如果是单一应用的服务器，只需要用iftop、sar等工具统计网卡流量就可以。如果服务器跑了多个应用，可以使用nethogs工具实现，它的特别之处在于可以显示每个进程的带宽占用情况，这样可以更直观获取网络使用情况。 7、说一下你们公司怎么发版的（代码怎么发布的）？ 笔者回答：我说什么来着，这个问题又问到了。发布：jenkins配置好代码路径（SVN或GIT），然后拉代码，打tag。需要编译就编译，编译之后推送到发布服务器（jenkins里面可以调脚本），然后从分发服务器往下分发到业务服务器上。 8、elk中的logstash是怎么收集日志的，在客户端的logstash配置文件主要有哪些内容？ 笔者回答：input、output两大块配置；input中指定日志（type、path）等，output指定日志输出的目标（host、port）等。 9、ansible你用过它的哪些模块，ansbile同时分发多台服务器的过程很慢（它是逐台分发的），你想过怎么解决吗？ 笔者回答：用过ansible的（copy file yum ping command shell）等模块；ansible默认只会创建5个进程,所以一次任务只能同时控制5台机器执行.那如果你有大量的机器需要控制,或者你希望减少进程数,那你可以采取异步执行.ansible的模块可以把task放进后台,然后轮询它.这使得在一定进程数下能让大量需要的机器同时运作起来. 10、nginx有哪几种调度算法，解释一下ip hash和轮询有啥不一样？ 笔者回答：常用的有3种调度算法（轮询、ip hash、权重）。 轮询：upstream按照轮询（默认）方式进行负载，每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 ip hash：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 权重：指定轮询几率，权重（weight）和访问比率成正比，用于后端服务器性能不均的情况。 11、nginx你用到了哪些模块，在proxy模块中你配置过哪些参数？ 笔者回答：用到过（负载均衡upstream、反向代理proxy_pass、location、rewrite等）。 proxy模块中配置过:proxy_set_header、proxy_connect_timeout、proxy_send_timeout、proxy_buffer_* 12、说一下iptables的原理，有哪些表、哪些链？怎么修改默认策略全部为DROP? 笔者回答：iptables是工作在TCP/IP的2、3、4层。你要说它的原理也不是几话能概括的，当主机收到一个数据包后，数据包先在内核空间中处理，若发现目的地址是自身，则传到用户空间中交给对应的应用程序处理，若发现目的不是自身，则会将包丢弃或进行转发。 4张表（raw表、mangle表、net表、filter表） 5条链（INPUT链、OUTPUT链、PORWARD链、PREROUTING链、POSTROUTING链）。 全部设置为DROP： #iptables -P INPUT DROP #iptables -P OUTPUT DROP #iptables -P FORWARD DROP 小结：iptables远不止这几句话就能描述清楚的，也不是随便在网上趴些资料就能学好的，需要自己用起来，经过大量的实验和实战才能熟悉它，iptables真的很考验运维人员的技术水平，大家一定要用心学好这个iptables。 13、如何开启linux服务器路由转发功能？ 笔者回答：echo “1” &gt; /proc/sys/net/ipv4/ip_forward 14、nginx中rewrite有哪几个flag标志位（last、break、redirect、permanent），说一下都什么意思？ 笔者回答： last : 相当于Apache的[L]标记，表示完成当前的rewrite规则break : 停止执行当前虚拟主机的后续rewrite指令集redirect : 返回302临时重定向，地址栏会显示跳转后的地址permanent : 返回301永久重定向，地址栏会显示跳转后的地址301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解： last一般写在server和if中，而break一般使用在location中last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后匹配break和last都能组织继续执行后面的rewrite指令总结：关于nginx rewrite用法，笔者看到一篇文章总结的挺不错 ，可以参考一下 https://www.jianshu.com/p/a1fce9358d44 15、你在shell脚本中用过哪些语法，case语法会用到哪些地方？ 笔者回答：一般会用到if语句、for语句、while语句、case语句以及function函数的定义；case语句为多选择语句，可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。最典型的case语法会用到启动服务脚本的处理。 16、linux系统中你会用到什么命令查看硬件使用状态信息？ 笔者回答：这个命令就很多了，比如：lscpu(查看cpu信息)、free -m（查看内存信息）、df -h（查看硬盘分区信息）、top（还可以动态查看cpu、内存使用情况的信息），/proc/目录下也可以查看很多硬件信息。 17、我要过滤一段文本(test.txt)中第二列的内容？如果这段文件有很多特殊符号，比如用:（冒号）怎么过滤它的第二段？如果我要过滤这段文本中，其中有一行只有7个符如何实现？ 笔者回答： awk ‘{print $2}’ tset.txt ​ awk -F’:’ ‘{print $2}’ tset.txt 18、比如开发想找你查看tomcat日志，但是catalia.out特别大，你不可能用vi打开去看，你会怎么查看？如果你用 grep -i”error” 过滤只是包含error的行，我想同时过滤error上面和下面的行如何实现？ 笔者回答： grep -i “error” catalia.out ​ grep -C 1 -i “error” catalia.out ​ 参数-C：是匹配前后的行，后面1是匹配前后各1行 19、 怎么编写一个定时计划任务？里面用到的最小单位是什么？ 笔者回答：crontab -e，最小单位是分钟 20、zabbix如何修改其中监控的一台服务器中内存阈值信息，比如正常内存使用到了80%报警，我想修改为60%报警？ 笔者回答：正常来说，一般会把监控的服务器统一加入到一个模板中，修改模板的其是某一项的监控项参数和告警阈值后，加入模板中的所有主机都会同步。如果单独想修改其中某一台服务器内存告警阈值，需要进入这台主机，单独创建一个告警Triggers，关联这台主机监控内存的项，配置好告警的阈值为60%即可实现。其实，zabbix一切都为图形化操作，如果没有接触过zabbix的朋友，可能听起来不太清楚。 21、mysql主从复制原理说一下？ 笔者回答：mysql支持三种复制类型（基于语句的复制、基于行的复制、混合类开进的复制）。 如果你记不住太多内容，可以简单说明一下原理： (1) master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； (2) slave将master的binary log events拷贝到它的中继日志(relay log)； (3) slave重做中继日志中的事件，将改变反映它自己的数据。 20180112090827.jpg 如果你能详细记住它的原理，可以这么回答： 该过程的第一部分就是master记录二进制日志。在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务。 下一步就是slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志。 SQL slave thread（SQL从线程）处理该过程的最后一步。SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致。只要该线程与I/O线程保持一致，中继日志通常会位于OS的缓存中，所以中继日志的开销很小。 22、用什么命令可以查看上一次服务器启动的时间、上一次谁登录过服务器？ 笔者回答：w命令查看上次服务器启动时间。last命令 查看登录。 23、redis集群原理说一下，正常情况下mysql有多个库，redis也有多个库，我怎么进入redis集群中的第2个库？还有，我想查看以BOSS开头的值？redis持久化是如何实现（一种是RDS、一种是AOF），说一下他们有啥不一样？ 笔者回答：这个redis原理的问题又问到了，看样子很多面试官都很关心这个redis，在上一篇文章笔者的一次面试也有这个面试问题。 【集群原理】：其实它的原理不是三两句话能说明白的，redis 3.0版本之前是不支持集群的，官方推荐最大的节点数量为1000，至少需要3(Master)+3(Slave)才能建立集群，是无中心的分布式存储架构，可以在多个节点之间进行数据共享，解决了Redis高可用、可扩展等问题。集群可以将数据自动切分(split)到多个节点，当集群中的某一个节点故障时，redis还可以继续处理客户端的请求。 【切库】：单机情况下用select 2可以切换第2个库，select 1可以切换第1个库。但是集群环境下不支持select。可参考https://yq.aliyun.com/articles/69349 【redis持久化】：持久化通俗来讲就是将内存中的数据写入硬盘中，redis提供了两种持久化的功能（RDB、AOF），默认使用RDB的方式。 RDB：全量写入持久化，而RDB持久化也分两种（SAVE、BGSAVE）。 SAVE是阻塞式的RDB持久化，当执行这个命令时redis的主进程把内存里的数据库状态写入到RDB文件（即上面的dump.rdb）中，直到该文件创建完毕的这段时间内redis将不能处理任何命令请求。 BGSAVE属于非阻塞式的持久化，它会创建一个子进程专门去把内存中的数据库状态写入RDB文件里，同时主进程还可以处理来自客户端的命令请求。但子进程基本是复制的父进程，这等于两个相同大小的redis进程在系统上运行，会造成内存使用率的大幅增加。 AOF：与RDB的保存整个redis数据库状态不同，AOF的持久化是通过命令追加、文件写入和文件同步三个步骤实现的。AOF是通过保存对redis服务端的写命令（如set、sadd、rpush）来记录数据库状态的，即保存你对redis数据库的写操作。 为了大家能够更好的理解redis持久化，笔者建议大家可以看下这两篇文章会比较好理解： https://www.cnblogs.com/Fairy-02-11/p/6182478.html http://blog.csdn.net/mishifangxiangdefeng/article/details/48977269 24、你在工作的过程中，遇到过你映像最深的是什么故障问题，你又是如何解决？ 笔者回答：这个问题主要也是考你排查故障的思路及用到的相关命令工具，其每个人在工作中都会遇到各种各样的问题（不管是网络问题、应用配置问题、还是APP打开慢/网站打开慢）等等。你只要记住一个你映像最为深刻、最为典型的故障就行。笔者也遇到过各种问题，我在这里就是写出来，怕误导了大家。 25、在linux服务器上，不管是用rz -y命令还是tftp工具上传，我把本地的一个文件上传到服务器完成后，服务器上还是什么都没有，这有可能是什么问题？ 笔者回答：根据这种现象有可能是：服务器磁盘满了；文件格式破坏了；或者你用的是普通用户上传，正好上传的目录没有权限；还有可能就是你上传的文件大小超出了该目录空间的范围。 26、你在工作中都写过什么脚本？ 笔者回答：这个问题的回答别把话说得太大了，要结合实际情况来回答。写过mysql、redis、mongodb等数据库备份的脚本；服务器文件备份的脚本；日常代码发布的脚本；之前用nagios的时候写过一些nagios插件的脚本。 27、rsync+inotify是实现文件实时同步的，加什么参数才能实现实时同步，–delete参数又是什么意思？ 笔者回答：rsync是远程同步工具、inotify是一种强大的异步文件系统系统监控机制。通过inotifywait 中的-m参数可以实现“始终保持事件监听状态”。rsync中的-delete参数是指“ 删除那些DST中SRC没有的文件”。 28、我想查看access.log中哪个IP访问最多？ 笔者回答：awk ‘{print $1}’ access.log| sort | uniq -c |sort -rn -k 1 | head -1 上面的具体参数如果有不知道的，大家可以自行百度一下，这里不说参数这么细节的问题 29、在linux系统中，一般都会有swap内存，你觉得使用swap内存有什么好处，在什么情况下swap内存才会被使用？你觉得在生产环境中要不要用swap内存？ 笔者回答：好处：在内存不够用的时候，将部分内存上的数据交换到swap空间上，以便让系统不会因为内存不够用而导致oom或者更致命的情况出现。 什么情况下会用swap：当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到swap空间中，等到那些程序要运行时，再从swap中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行swap交换。 30、怎么查看两台服务器之间的网络是不是正常的，服务器是禁ping的？ 笔者回答：不能用ping，那可以用telnet对方服务器的端口、或者互相访问对方打开的服务。其它的测试方法笔者也没想到，要是哪位朋友有好的方法不访在下面留言讨论。 31、比如我访问百度网站，有什么方法可以跟踪经过了哪些网络节点？ 笔者回答：这个太简单了吧，干运维必备的网络排查技能。用tracert命令就可以跟踪，主要是查询本机到另一个主机经过的路由跳数及数据延迟情况。然后你也可以把具体跟踪后输出的信息也说出来，你能说出来都是为你加分的。 32、如果你们公司的网站访问很慢，你会如何排查？ 笔者回答：看到没有，又问到了这个问题，笔者在上一篇文章 2019年2月14号的面试中面试官也问到同样的问题。其实这种问题都没有具体答案，只是看你回答的内容与面试官契合度有多高，能不能说到他想要的点上，主要是看你排查问题的思路。我是这么说的：问清楚反应的人哪个服务应用或者页面调取哪个接口慢，叫他把页面或相关的URL发给你，首先，最直观的分析就是用浏览器按F12，看下是哪一块的内容过慢（DNS解析、网络加载、大图片、还是某个文件内容等），如果有，就对症下药去解决（图片慢就优化图片、网络慢就查看内网情况等）。其次，看后端服务的日志，其实大多数的问题看相关日志是最有效分析，最好用tail -f 跟踪一下日志，当然你也要点击测试来访问接口日志才会打出来。最后，排除sql，，找到sql去mysql执行一下，看看时间是否很久，如果很久，就要优化SQL问题了，expain一下SQL看看索引情况啥的，针对性优化。数据量太大的能分表就分表，能分库就分库。如果SQL没啥问题，那可能就是写的逻辑代码的问题了，一行行审代码，找到耗时的地方改造，优化逻辑。 33、我需要查看某个时间段的日志(比如access.log日志)，如何实现? 笔者回答：方法有很多种，比如我要看查的时间是2019年1月9号–1月10号的日志吧。 比如可以用sed命令，格式为：sed -n ‘/起始时间/,/结束时间/p’ 日志文件，如下： sed -n ‘/09/Jan/2019/,/10/Jan/2019/p’ access.log 比如可以用grep，格式为：grep -E ‘起始时间|结束时间’ 日志文件，如下： grep -E ‘09/Jan/2018|10/Jan/2018’ access.log 当然，你还可以结合cat、grep 、awk这些命令一起来使用都行","categories":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://LTSpider.github.io/tags/运维/"}],"keywords":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}]},{"title":"增删改","slug":"增删改","date":"2019-07-26T04:17:52.674Z","updated":"2019-07-29T08:21:24.059Z","comments":true,"path":"2019/07/26/增删改/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/增删改/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031from pymysql import *def main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,database='jing_dong',user='root',password='mysql',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # 执行insert语句，并返回受影响的行数：添加一条数据 # 增加 count = cs1.execute('insert into goods_cates(name) values(\"硬盘\")') #打印受影响的行数 print(count) count = cs1.execute('insert into goods_cates(name) values(\"光盘\")') print(count) # # 更新 # count = cs1.execute('update goods_cates set name=\"机械硬盘\" where name=\"硬盘\"') # # 删除 # count = cs1.execute('delete from goods_cates where id=6') # 提交之前的操作，如果之前已经之执行过多次的execute，那么就都进行提交 conn.commit() # 关闭Cursor对象 cs1.close() # 关闭Connection对象 conn.close()if __name__ == '__main__': main() 查询一行数据12345678910111213141516171819202122232425from pymysql import *def main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,user='root',password='mysql',database='jing_dong',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # 执行select语句，并返回受影响的行数：查询一条数据 count = cs1.execute('select id,name from goods where id&gt;=4') # 打印受影响的行数 print(\"查询到%d条数据:\" % count) for i in range(count): # 获取查询的结果 result = cs1.fetchone() # 打印查询的结果 print(result) # 获取查询的结果 # 关闭Cursor对象 cs1.close() conn.close()if __name__ == '__main__': main() 查询多行数据12345678910111213141516171819202122232425262728from pymysql import *def main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,user='root',password='mysql',database='jing_dong',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # 执行select语句，并返回受影响的行数：查询一条数据 count = cs1.execute('select id,name from goods where id&gt;=4') # 打印受影响的行数 print(\"查询到%d条数据:\" % count) # for i in range(count): # # 获取查询的结果 # result = cs1.fetchone() # # 打印查询的结果 # print(result) # # 获取查询的结果 result = cs1.fetchall() print(result) # 关闭Cursor对象 cs1.close() conn.close()if __name__ == '__main__': main()","categories":[{"name":"backend","slug":"backend","permalink":"http://LTSpider.github.io/categories/backend/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://LTSpider.github.io/tags/Python/"},{"name":"pymsql","slug":"pymsql","permalink":"http://LTSpider.github.io/tags/pymsql/"}],"keywords":[{"name":"backend","slug":"backend","permalink":"http://LTSpider.github.io/categories/backend/"}]},{"title":"sql 参数化","slug":"参数化","date":"2019-07-26T04:17:52.671Z","updated":"2019-07-29T08:22:40.367Z","comments":true,"path":"2019/07/26/参数化/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/参数化/","excerpt":"","text":"sql语句的参数化，可以有效防止sql注入注意：此处不同于python的字符串格式化，全部使用%s占位 1234567891011121314151617181920212223242526272829303132333435363738394041from pymysql import *def main(): find_name = input(\"请输入物品名称：\") # 创建Connection连接 conn = connect(host='localhost',port=3306,user='root',password='mysql',database='jing_dong',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # # 非安全的方式 # # 输入 \" or 1=1 or \" (双引号也要输入) # sql = 'select * from goods where name=\"%s\"' % find_name # print(\"\"\"sql===&gt;%s&lt;====\"\"\" % sql) # # 执行select语句，并返回受影响的行数：查询所有数据 # count = cs1.execute(sql) # 安全的方式 # 构造参数列表 params = [find_name] # 执行select语句，并返回受影响的行数：查询所有数据 count = cs1.execute('select * from goods where name=%s', params) # 注意： # 如果要是有多个参数，需要进行参数化 # 那么params = [数值1, 数值2....]，此时sql语句中有多个%s即可 # 打印受影响的行数 print(count) # 获取查询的结果 # result = cs1.fetchone() result = cs1.fetchall() # 打印查询的结果 print(result) # 关闭Cursor对象 cs1.close() # 关闭Connection对象 conn.close()if __name__ == '__main__': main()","categories":[{"name":"backend","slug":"backend","permalink":"http://LTSpider.github.io/categories/backend/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://LTSpider.github.io/tags/Python/"},{"name":"sql","slug":"sql","permalink":"http://LTSpider.github.io/tags/sql/"}],"keywords":[{"name":"backend","slug":"backend","permalink":"http://LTSpider.github.io/categories/backend/"}]},{"title":"Win10下使用Docker部署Mongodb","slug":"Win10下使用Docker部署mongo","date":"2019-07-26T04:17:52.667Z","updated":"2019-07-29T08:23:47.689Z","comments":true,"path":"2019/07/26/Win10下使用Docker部署mongo/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/Win10下使用Docker部署mongo/","excerpt":"","text":"在window下部署mongo，实在是有些坑。网上的很多教程是基于liunx，一般不会碰到很多麻烦。但在win10下就可能会出现问题。 1234注册账号，安装Docker获取Mongo镜像挂接数据目录启动容器启动容器 1、注册账号，安装Docker已经安装的可以忽略。 1官网下载Docker： https://www.docker.com/get-started 安装完成后，托盘下有个小鲸鱼。在cmd命令行下输入docker version，可以看到Docker的版本信息。 1234567891011121314151617181920D:\\ISOλ docker versionClient: Docker Engine - Community Version: 18.09.2 API version: 1.39 Go version: go1.10.8 Git commit: 6247962 Built: Sun Feb 10 04:12:31 2019 OS/Arch: windows/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.2 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 6247962 Built: Sun Feb 10 04:13:06 2019 OS/Arch: linux/amd64 Experimental: false 2、获取Mongo镜像使用搜索命令docker search &lt;名称&gt;可以找到一堆相关镜像。OFFICIAL = [OK]是官方的。 12345678910111213141516171819202122232425262728D:\\ISOλ docker search mongoNAME DESCRIPTION STARS OFFICIAL AUTOMATEDmongo MongoDB document databases provide high avai… 6033 [OK]mongo-express Web-based MongoDB admin interface, written w… 487 [OK]tutum/mongodb MongoDB Docker image – listens in port 27017… 226 [OK]bitnami/mongodb Bitnami MongoDB Docker Image 92 [OK]mongoclient/mongoclient Official docker image for Mongoclient, featu… 70 [OK]mongooseim/mongooseim Small docker image for MongooseIM - robust a… 18frodenas/mongodb A Docker Image for MongoDB 17 [OK]cvallance/mongo-k8s-sidecar Kubernetes side car to setup and maintain a … 11 [OK]centos/mongodb-32-centos7 MongoDB NoSQL database server 7centos/mongodb-26-centos7 MongoDB NoSQL database server 5istepanov/mongodump Docker image with mongodump running as a cro… 5 [OK]centos/mongodb-36-centos7 MongoDB NoSQL database server 4eses/mongodb_exporter mongodb exporter for prometheus 4 [OK]webhippie/mongodb Docker images for MongoDB 4 [OK]circleci/mongo CircleCI images for MongoDB 4 [OK]arm64v8/mongo MongoDB document databases provide high avai… 4requilence/mongodb-backup mongo backup container 3 [OK]neowaylabs/mongodb-mms-agent This Docker image with MongoDB Monitoring Ag… 2 [OK]centos/mongodb-34-centos7 MongoDB NoSQL database server 2ekesken/mongo docker image for mongo that is configurable … 1 [OK]openshift/mongodb-24-centos7 DEPRECATED: A Centos7 based MongoDB v2.4 ima… 1ansibleplaybookbundle/mongodb-apb An APB to deploy MongoDB. 0 [OK]martel/mongo-replica-ctrl A dockerized controller for a Mongo db repli… 0 [OK]andreasleicher/mongo-azure-backup a docker container to backup a mongodb using… 0 [OK]fuww/mongo-connector mongo-connector + alpine + docker 0 [OK] 直接抓取最新版的Mongo，如果需要特定版本可以在后面加版本号。 123D:\\&gt;docker pull mongo抓取V 3.2版本的MongoD:\\&gt;docker pull mongo:3.2 完成后可以看到本地镜像 12345D:\\ISOλ docker image ls -aREPOSITORY TAG IMAGE ID CREATED SIZEmongo latest 785c65f61380 12 days ago 412MBubuntu latest 4c108a37151f 3 weeks ago 64.2MB 3、挂接数据目录启动容器docker本身的镜像都属于只读，要保存数据就需要找个地方存。 注意： 这个数据目录挂接在win10下是个坑 通常情况是建个目录，如（d:\\dockerdata\\mongo）使用 -v 命令参数进行连接，网上大部分的教程都是如此。实际会出现权限问题，无法正常启动。巨坑无比，浪费了好多时间 解决： 使用数据卷（Volume）解决，可以理解位虚拟磁盘。 3.1、创建数据卷1234567D:\\ISOλ docker volume create --name mongodatamongodataD:\\ISOλ docker volume lsDRIVER VOLUME NAMElocal mongodata 可以看到已经创建了一个mongodata 的数据卷。 单独删除： docker volume rm &lt;名称&gt; 3.2、挂接运行mongo数据库123456D:\\&gt;docker run --name mongodb -v mongodata:/data/db -p 27017:27017 -d mongo:latest --autha0585181d6102c6c4e1ebd7686fc8d08827632b5b279fb4eae7bf746e8ea49a9D:\\&gt;docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa0585181d610 mongo:latest \"docker-entrypoint.s…\" 11 seconds ago Up 9 seconds 0.0.0.0:27017-&gt;27017/tcp mongodb 参数：docker run 运行容器–name mongodb 运行容器的名称为mongodb-v mongodata:/data/db 挂接保存数据的位置，冒号前面是本机（mongodata），后面是虚拟机中的映射目录（/data/db）-p 27017:27017 映射端口，前面是本机端口，后面是docker内的端口–auth 授权访问 命令： docker ps 查看当前正在运行的容器对象，Mongo容器运行正常。 4、授权创建账户 命令： docker exec -it mongodb mongo admin 123456789101112D:\\&gt;docker exec -it mongodb mongo adminMongoDB shell version v4.0.3connecting to: mongodb://127.0.0.1:27017/adminImplicit session: session &#123; \"id\" : UUID(\"85d97306-33e4-45d2-a8a5-ca85a2b46165\") &#125;MongoDB server version: 4.0.3Welcome to the MongoDB shell.For interactive help, type \"help\".For more comprehensive documentation, see http://docs.mongodb.org/Questions? Try the support group http://groups.google.com/group/mongodb-user&gt; 创建账号 12345db.createUser(&#123; user: &apos;root&apos;, pwd: &apos;admin&apos;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;); 授权：1 代表授权验证成功 1db.auth(&quot;root&quot;,&quot;admin&quot;); 创建一个新的数据库 blog_db，并授权 12345678910创建目标数据库（实际上切换即可）use blog_db创建目标数据库管理用户db.createUser(&#123; user: &apos;blog&apos;, pwd: &apos;blog123456&apos;, roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;blog_db&quot; &#125; ] &#125;);开启验证db.auth(&quot;blog&quot;,&quot;blog123456&quot;); OK搞定在win10下使用Docker有时有点坑，网上很多教程都是基于liunx的操作步骤。 现在碰到的主要有两个问题： 1、docker search 没有反应2、系统没授权 Operation not permitted 相关问题1：docker search 没有反应在使用docker中，正常都是设置自动启动Docker。在启动电脑后会出现pull无法拉取镜像 12D:&gt;docker search mongoError response from daemon: Get https://index.docker.io/v1/search?q=mongo&amp;n=25: dial tcp: lookup index.docker.io on 192.168.65.1:53: read udp 192.168.65.3:44014-&gt;192.168.65.1:53: i/o timeout 解决方法：1、重新启动Docker，点托盘的小图标，选择重启菜单2、重启无法解决，加国内镜像 增加国内镜像： 点击托盘小图标 选择 settings 菜单 在Settings对话框左侧选择 Daemon 选项 右侧 Registry mirrors 填入镜像地址 重启Docker 镜像地址1234随便加一个，都加也没问题https://docker.mirrors.ustc.edu.cnhttp://hub-mirror.c.163.comhttps://registry.docker-cn.com 相关问题2： 系统授权 Operation not permitted使用常规的磁盘映射会出现的问题，如把数据文件映射到 D:\\docker\\data\\mongo 目录下，使用 docker ps 就会发现没有容器在运行 12345C:\\Users\\sunseeds&gt;docker run --name mongodb-server0 -v D:\\docker\\data\\mongo:/data/db -p 27017:27017 -d mongo:latest --auth1a1ec10c64716a8f2daee89ab7a18068559a0e7c2460741be0cc5cf82809812dC:\\Users\\sunseeds&gt;docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 使用下面命令docker ps -a，发现容器创建了，但未能正常运行。 1docker ps -a 用 docker logs mongodb-server0 查看日志，启动失败无法创建数据文件的元凶，一堆密密麻麻的字里有一堆的Operation not permitted，最后来一句失败！ 1234567891011121314151617181920D:&gt;docker logs mongodb-server02018-10-17T13:59:14.506+0000 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'2018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=1a1ec10c64712018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] db version v4.0.32018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] git version: 7ea530946fa7880364d88c8d8b6026bbc9ffa48c2018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.2g 1 Mar 20162018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] allocator: tcmalloc2018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] modules: none2018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] build environment:2018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] distmod: ubuntu16042018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] distarch: x86_642018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] target_arch: x86_642018-10-17T13:59:14.512+0000 I CONTROL [initandlisten] options: &#123; net: &#123; bindIpAll: true &#125;, security: &#123; authorization: \"enabled\" &#125; &#125;2018-10-17T13:59:14.525+0000 I STORAGE [initandlisten] wiredtiger_open config: create,cache_size=478M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=(recovery_progress),2018-10-17T13:59:15.354+0000 E STORAGE [initandlisten] WiredTiger error (1) [1539784755:354808][1:0x7f0386d16a00], connection: __posix_open_file, 715: /data/db/WiredTiger.wt: handle-open: open: Operation not permitted Raw: [1539784755:354808][1:0x7f0386d16a00], connection: __posix_open_file, 715: /data/db/WiredTiger.wt: handle-open: open: Operation not permitted2018-10-17T13:59:15.405+0000 E STORAGE [initandlisten] WiredTiger error (17) [1539784755:405544][1:0x7f0386d16a00], connection: __posix_open_file, 715: /data/db/WiredTiger.wt: handle-open: open: File exists Raw: [1539784755:405544][1:0x7f0386d16a00], connection: __posix_open_file, 715: /data/db/WiredTiger.wt: handle-open: open: File exists... ...2018-10-17T13:59:15.461+0000 F - [initandlisten]***aborting after fassert() failure 解决： 创建Volume，进行连接 Volume可以看作是Docker创建的一个虚拟磁盘。","categories":[{"name":"database","slug":"database","permalink":"http://LTSpider.github.io/categories/database/"}],"tags":[{"name":"Docker Mongo","slug":"Docker-Mongo","permalink":"http://LTSpider.github.io/tags/Docker-Mongo/"}],"keywords":[{"name":"database","slug":"database","permalink":"http://LTSpider.github.io/categories/database/"}]},{"title":"ssh用法及命令","slug":"SSH登录2","date":"2019-07-26T04:17:52.664Z","updated":"2019-07-29T08:23:29.820Z","comments":true,"path":"2019/07/26/SSH登录2/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/SSH登录2/","excerpt":"","text":"什么是SSH？简单说，SSH是一种网络协议，用于计算机之间的加密登录。如果一个用户从本地计算机，使用SSH协议登录另一台远程计算机，我们就可以认为，这种登录是安全的，即使被中途截获，密码也不会泄露。最早的时候，互联网通信都是明文通信，一旦被截获，内容就暴露无疑。1995年，芬兰学者Tatu Ylonen设计了SSH协议，将登录信息全部加密，成为互联网安全的一个基本解决方案，迅速在全世界获得推广，目前已经成为Linux系统的标准配置。SSH只是一种协议，存在多种实现，既有商业实现，也有开源实现。本文针对的实现是OpenSSH，它是自由软件，应用非常广泛。这里只讨论SSH在Linux Shell中的用法。如果要在Windows系统中使用SSH，会用到另一种软件PuTTY，这需要另文介绍。 中间人攻击SSH之所以能够保证安全，原因在于它采用了公钥加密。整个过程是这样的：（1）远程主机收到用户的登录请求，把自己的公钥发给用户。（2）用户使用这个公钥，将登录密码加密后，发送回来。（3）远程主机用自己的私钥，解密登录密码，如果密码正确，就同意用户登录。这个过程本身是安全的，但是实施的时候存在一个风险：如果有人截获了登录请求，然后冒充远程主机，将伪造的公钥发给用户，那么用户很难辨别真伪。因为不像https协议，SSH协议的公钥是没有证书中心（CA）公证的，也就是说，都是自己签发的。可以设想，如果攻击者插在用户与远程主机之间（比如在公共的wifi区域），用伪造的公钥，获取用户的登录密码。再用这个密码登录远程主机，那么SSH的安全机制就荡然无存了。这种风险就是著名的”中间人攻击”（Man-in-the-middle attack）。 ssh的安装SSH分客户端openssh-client和openssh-server 如果你只是想登陆别的机器的SSH只需要安装openssh-client（ubuntu有默认安装，如果没有则sudoapt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。 Ubuntu缺省已经安装了ssh client。 #配置ssh123456789101112131415161718192021echo -e &quot;\\033[31;1m ******************************* \\033[0m&quot;echo -e &quot;\\033[31;1m ************安装和配置ssh************ \\033[0m&quot;sudo apt-get install -y openssh-server 1&gt; /dev/nullsudo sed -i &apos;s/UsePAM no/UsePAM yes/g&apos; /etc/ssh/sshd_configsudo sed -i &apos;8a /etc/init.d/ssh start&apos; /etc/profilesudo /etc/init.d/ssh startps -e | grep ssh echo -e &quot;\\033[31;1m ssh授权 \\033[0m&quot;cd ~/.ssh/ssh-keygen -t rsacat ./id_rsa.pub &gt;&gt; ./authorized_keys$ ps -e|grep ssh 2151 ? 00:00:00 ssh-agent 5313 ? 00:00:00 sshdssh-agent表示ssh-client启动，sshd表示ssh-server启动了。如果缺少sshd，说明ssh服务没有启动或者没有安装。 SSH基本用法SSH远程登录口令登录假定你要以用户名user，登录远程主机host，只要一条简单命令就可以了。 1$ ssh user@host 如：ssh pika@192.168.0.111 如果本地用户名与远程用户名一致，登录时可以省略用户名。 1$ ssh host SSH的默认端口是22，也就是说，你的登录请求会送进远程主机的22端口。使用p参数，可以修改这个端口。 1$ ssh -p 2222 user@host上面这条命令表示，ssh直接连接远程主机的2222端口。如果你是第一次登录对方主机，系统会出现下面的提示： 1234$ ssh user@host The authenticity of host 'host (12.18.429.21)' can't be established. RSA key fingerprint is 98:2e:d7:e0:de:9f:ac:67:28:c2:42:2d:37:16:58:4d. Are you sure you want to continue connecting (yes/no)? 这段话的意思是，无法确认host主机的真实性，只知道它的公钥指纹，问你还想继续连接吗？所谓”公钥指纹”，是指公钥长度较长（这里采用RSA算法，长达1024位），很难比对，所以对其进行MD5计算，将它变成一个128位的指纹。上例中是98:2e:d7:e0:de:9f:ac:67:28:c2:42:2d:37:16:58:4d，再进行比较，就容易多了。很自然的一个问题就是，用户怎么知道远程主机的公钥指纹应该是多少？回答是没有好办法，远程主机必须在自己的网站上贴出公钥指纹，以便用户自行核对。假定经过风险衡量以后，用户决定接受这个远程主机的公钥。 Are you sure you want to continue connecting (yes/no)? yes系统会出现一句提示，表示host主机已经得到认可。 Warning: Permanently added ‘host,12.18.429.21’ (RSA) to the list of known hosts.然后，会要求输入密码。 Password: (enter password)如果密码正确，就可以登录了。当远程主机的公钥被接受以后，它就会被保存在文件$HOME/.ssh/known_hosts之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示输入密码。每个SSH用户都有自己的known_hosts文件，此外系统也有一个这样的文件，通常是/etc/ssh/ssh_known_hosts，保存一些对所有用户都可信赖的远程主机的公钥。 公钥登录使用密码登录，每次都必须输入密码，非常麻烦。好在SSH还提供了公钥登录，可以省去输入密码的步骤。所谓”公钥登录”，原理很简单，就是用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。这种方法要求用户必须提供自己的公钥。如果没有现成的，可以直接用ssh-keygen生成一个： 1$ ssh-keygen运行上面的命令以后，系统会出现一系列提示，可以一路回车。其中有一个问题是，要不要对私钥设置口令（passphrase），如果担心私钥的安全，这里可以设置一个。运行结束以后，在$HOME/.ssh/目录下，会新生成两个文件：id_rsa.pub和id_rsa。前者是你的公钥，后者是你的私钥。这时再输入下面的命令，将公钥传送到远程主机host上面： 1$ ssh-copy-id user@host 好了，从此你再登录，就不需要输入密码了。如果还是不行，就打开远程主机的/etc/ssh/sshd_config这个文件，检查下面几行前面”#”注释是否取掉。 RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys然后，重启远程主机的ssh服务。 1234// ubuntu系统 service ssh restart// debian系统 /etc/init.d/ssh restartauthorized_keys文件 远程主机将用户的公钥，保存在登录后的用户主目录的$HOME/.ssh/authorized_keys文件中。公钥就是一段字符串，只要把它追加在authorized_keys文件的末尾就行了。这里不使用上面的ssh-copy-id命令，改用下面的命令，解释公钥的保存过程： 1$ ssh user@host 'mkdir -p .ssh &amp;&amp; cat &gt;&gt; .ssh/authorized_keys' &lt; ~/.ssh/id_rsa.pub 这条命令由多个语句组成，依次分解开来看：（1）”$ ssh user@host”，表示登录远程主机；（2）单引号中的mkdir .ssh &amp;&amp; cat &gt;&gt; .ssh/authorized_keys，表示登录后在远程shell上执行的命令：（3）”$ mkdir -p .ssh”的作用是，如果用户主目录中的.ssh目录不存在，就创建一个；（4）’cat &gt;&gt; .ssh/authorized_keys’ &lt; /.ssh/id_rsa.pub的作用是，将本地的公钥文件/.ssh/id_rsa.pub，重定向追加到远程文件authorized_keys的末尾。写入authorized_keys文件后，公钥登录的设置就完成了。 [SSH原理与运用（一）：远程登录]使用ssh在远程后台不中断地跑程序Linux关闭ssh（关闭终端等）后运行的程序或者服务自动停止，如python3 a.py &amp;。 解决：使用nohup命令让程序在关闭窗口（切换SSH连接）的时候程序还能继续在后台运行。 nohup python3 a.py &amp; [linux进程管理与SELinux]SSH远程操作SSH数据传输SSH不仅可以用于远程主机登录，还可以直接在远程主机上执行操作。 1$ ssh user@host 'mkdir -p .ssh &amp;&amp; cat &gt;&gt; .ssh/authorized_keys' &lt; ~/.ssh/id_rsa.pub 单引号中间的部分，表示在远程主机上执行的操作；后面的输入重定向，表示数据通过SSH传向远程主机。这就是说，SSH可以在用户和远程主机之间，建立命令和数据的传输通道，因此很多事情都可以通过SSH来完成。下面看几个例子。【例1】将$HOME/src/目录下面的所有文件，复制到远程主机的$HOME/src/目录。 1$ cd &amp;&amp; tar czv src | ssh user@host 'tar xz' 【例2】将远程主机$HOME/src/目录下面的所有文件，复制到用户的当前目录。 1$ ssh user@host 'tar cz src' | tar xzv 【例3】查看远程主机是否运行进程httpd。 1$ ssh user@host 'ps ax | grep [h]ttpd' lz建议使用scp进行远程copy： scp 跨机远程拷贝scp是secure copy的简写，用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。两台主机之间复制文件必需得同时有两台主机的复制执行帐号和操作权限。 scp命令参数1234567891011121314151617-1 强制scp命令使用协议ssh1-2 强制scp命令使用协议ssh2-4 强制scp命令只使用IPv4寻址-6 强制scp命令只使用IPv6寻址-B 使用批处理模式（传输过程中不询问传输口令或短语）-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）-p 留原文件的修改时间，访问时间和访问权限。-q 不显示传输进度条。-r 递归复制整个目录。-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。-l limit 限定用户所能使用的带宽，以Kbit/s为单位。-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，-P port 注意是大写的P, port是指定数据传输用到的端口号-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。 scp一般有六种使用方法1234567891011121314151617181920本地复制远程文件：（把远程的文件复制到本地）scp root@www.test.com:/val/test/test.tar.gz /val/test/test.tar.gz远程复制本地文件：（把本地的文件复制到远程主机上）scp /val/test.tar.gz root@www.test.com:/val/test.tar.gz本地复制远程目录：（把远程的目录复制到本地）scp -r root@www.test.com:/val/test/ /val/test/远程复制本地目录：（把本地的目录复制到远程主机上）scp -r ./ubuntu_env/ root@192.168.0.111:/home/pipipika:/media/pika/files/machine_learning/datasets$scp -r SocialNetworks/ piting@192.168.0.172:/media/data/pipi/datasets本地复制远程文件到指定目录：（把远程的文件复制到本地）scp root@www.test.com:/val/test/test.tar.gz /val/test/远程复制本地文件到指定目录：（把本地的文件复制到远程主机上）scp /val/test.tar.gz root@www.test.com:/val/ps: scp复制文件时只指定服务器地址不加路径默认复制到哪里???[12个scp传输文件的命令栗子][scp 跨机远程拷贝] SSH端口操作绑定本地端口既然SSH可以传送数据，那么我们可以让那些不加密的网络连接，全部改走SSH连接，从而提高安全性。假定我们要让8080端口的数据，都通过SSH传向远程主机，命令就这样写： 1$ ssh -D 8080 user@host SSH会建立一个socket，去监听本地的8080端口。一旦有数据传向那个端口，就自动把它转移到SSH连接上面，发往远程主机。可以想象，如果8080端口原来是一个不加密端口，现在将变成一个加密端口。 本地端口转发有时，绑定本地端口还不够，还必须指定数据传送的目标主机，从而形成点对点的”端口转发”。为了区别后文的”远程端口转发”，我们把这种情况称为”本地端口转发”（Local forwarding）。假定host1是本地主机，host2是远程主机。由于种种原因，这两台主机之间无法连通。但是，另外还有一台host3，可以同时连通前面两台主机。因此，很自然的想法就是，通过host3，将host1连上host2。我们在host1执行下面的命令： 1$ ssh -L 2121:host2:21 host3 命令中的L参数一共接受三个值，分别是”本地端口:目标主机:目标主机端口”，它们之间用冒号分隔。这条命令的意思，就是指定SSH绑定本地端口2121，然后指定host3将所有的数据，转发到目标主机host2的21端口（假定host2运行FTP，默认端口为21）。这样一来，我们只要连接host1的2121端口，就等于连上了host2的21端口。 1$ ftp localhost:2121 “本地端口转发”使得host1和host3之间仿佛形成一个数据传输的秘密隧道，因此又被称为”SSH隧道”。下面是一个比较有趣的例子。 1$ ssh -L 5900:localhost:5900 host3 它表示将本机的5900端口绑定host3的5900端口（这里的localhost指的是host3，因为目标主机是相对host3而言的）。另一个例子是通过host3的端口转发，ssh登录host2。 1$ ssh -L 9001:host2:22 host3 这时，只要ssh登录本机的9001端口，就相当于登录host2了。 1$ ssh -p 9001 localhost 上面的-p参数表示指定登录端口。 出错处理：ssh: Could not resolve hostname 192.168..:***: Name or service not known 解决：指定端口不能直接使用ip:端口号，使用-p参数来解决就可以了。 远程端口转发既然”本地端口转发”是指绑定本地端口的转发，那么”远程端口转发”（remote forwarding）当然是指绑定远程端口的转发。还是接着看上面那个例子，host1与host2之间无法连通，必须借助host3转发。但是，特殊情况出现了，host3是一台内网机器，它可以连接外网的host1，但是反过来就不行，外网的host1连不上内网的host3。这时，”本地端口转发”就不能用了，怎么办？解决办法是，既然host3可以连host1，那么就从host3上建立与host1的SSH连接，然后在host1上使用这条连接就可以了。我们在host3执行下面的命令： 1$ ssh -R 2121:host2:21 host1 R参数也是接受三个值，分别是”远程主机端口:目标主机:目标主机端口”。这条命令的意思，就是让host1监听它自己的2121端口，然后将所有数据经由host3，转发到host2的21端口。由于对于host3来说，host1是远程主机，所以这种情况就被称为”远程端口绑定”。绑定之后，我们在host1就可以连接host2了： 1$ ftp localhost:2121 这里必须指出，”远程端口转发”的前提条件是，host1和host3两台主机都有sshD和ssh客户端。 SSH的其他参数SSH还有一些别的参数，也值得介绍。N参数，表示只连接远程主机，不打开远程shell；T参数，表示不为这个连接分配TTY。这个两个参数可以放在一起用，代表这个SSH连接只用来传数据，不执行远程操作。 1$ ssh -NT -D 8080 host f参数，表示SSH连接成功后，转入后台运行。这样一来，你就可以在不中断SSH连接的情况下，在本地shell中执行其他操作。 1$ ssh -f -D 8080 host 要关闭这个后台连接，就只有用kill命令去杀掉进程。 [SSH原理与运用（二）：远程操作与端口转发]ref: [Ubuntu环境下SSH的安装及使用][25个必须记住的SSH命令]*[Linux 下 SSH 命令实例指南]*[数字签名是什么？][ * SSH, The Secure Shell: The Definitive Guide: 2.4. Authentication by Cryptographic Key, O’reilly SSH, The Secure Shell: The Definitive Guide: 9.2. Port Forwarding, O’reilly Shebang: Tips for Remote Unix Work (SSH, screen, and VNC) brihatch: SSH Host Key Protection brihatch: SSH User Identities IBM developerWorks: 实战 SSH 端口转发 Jianing YANG：ssh隧道技术简介 WikiBooks: Internet Technologies/SSH Buddhika Chamith: SSH Tunneling Explained ]1原文来源：https://blog.csdn.net/pipisorry/article/details/52269785","categories":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}],"tags":[{"name":"SSH  Linux","slug":"SSH-Linux","permalink":"http://LTSpider.github.io/tags/SSH-Linux/"}],"keywords":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}]},{"title":"Shell脚本之查看磁盘目录、删除用户脚本","slug":"Shell编程之道","date":"2019-07-26T04:17:52.656Z","updated":"2019-07-29T08:23:09.913Z","comments":true,"path":"2019/07/26/Shell编程之道/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/Shell编程之道/","excerpt":"","text":"1234567891011121314151617181920#!/usr/bin/env bash# 来源: https://blog.csdn.net/qianghaohao/article/details/80379118 # 服务器线程数达到 2500 以上时 dump 线程数最多的 java 进程的线程及内存#source ~/.bashrccur_thread_num=`ps -efL | wc -l`if [ $cur_thread_num -le 2500 ]; then exit 0ficur_date=`date +\"%Y-%m-%d_%H-%M-%S\"`cd ./dumpfile# 服务器当前线程 dump 到文件:按照线程数由大到小排序显示ps -efL --sort -nlwp &gt; server_thread_dump_$cur_date# dump 线程数最多的 jvm 的线程及内存most_thread_num_pid=`cat server_thread_dump_$cur_date | sed -n '2p' | awk '&#123;print $2&#125;'`nohup jstack -l $most_thread_num_pid &gt; java_app_thread_dump_$&#123;cur_date&#125;_pid_$&#123;most_thread_num_pid&#125; &amp;nohup jmap -dump:format=b,file=java_app_mem_dump_$&#123;cur_date&#125;_pid_$&#123;most_thread_num_pid&#125; $most_thread_num_pid &amp;exit 0 查看磁盘目录使用情况12345678910111213141516171819202122232425262728293031#!/bin/bash## Big_Users - find big disk space users in various directories##############################################################Parameters for script#CHECK_DIRECTORIES=\"/var/log /home\" #directories to check########################## Main Script ########################DATE=$(date '+%m%d%y') #Date for report file#exec &gt; disk_space_$DATE.rpt #Make report file Std Output#echo \"Top Ten Disk Space Usage\" #Report header for while reportecho \"for $CHECK_DIRECTORIES Directories\"#for DIR_CHECK in $CHECK_DIRECTORIES #loop to du directoriesdo echo \"\" echo \"The $DIR_CHECK Directory:\" #Title header for each directory## Creating a listing of top ten disk space users du -S $DIR_CHECK 2&gt;/dev/null | sort -rn | sed '&#123;11,$D; =&#125;' | sed 'N; s/\\n/ /' | gawk '&#123;printf $1 \":\" \"\\t\" $2 \"\\t\" $3 \"\\n\"&#125;'#done #End of for loop for du directories# 删除用户脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247#!/bin/bash## Delete_User - Automates the 4 steps to remove an account################################################################### # Define Functions##################################################################function get_answer &#123;# unset ANSWER ASK_COUNT=0# while [ -z \"$ANSWER\" ] # while no answer is given, keep asking do ASK_COUNT=$[ $ASK_COUNT + 1 ]# case $ASK_COUNT in # If user gives no answer in time allowed 2) echo echo \"Please answer the question.\" echo ;; 3) echo echo \"One last try... please answer the question.\" echo ;; 4) echo echo \"Since you refuse to answer the question...\" echo \"exiting program.\" echo # exit ;; esac# echo# if [ -n \"$LINE2\" ] then echo $LINE1 # Print 2 lines echo -e $LINE2\" \\c\" else # Print 1 line echo -e $LINE1\" \\c\" fi## Allow 60 seconds to answer before time-out read -t 60 ANSWER done## Do a little variable clean-up# unset LINE1 unset LINE2#&#125; #end of get_answer function##################################################################function process_answer &#123;# case $ANSWER in y|Y|YES|yes|yEs|yeS|YEs|yES) # If user answers \"yes\".do nothing. ;; *) # If user answers anything but \"yes\", exit script echo echo $EXIT_LINE1 echo $EXIT_LINE2 echo exit ;; esac # # Do a little variable clean-up unset EXIT_LINE1 unset EXIT_LINE2#&#125; #End of process_answer function################################################################### End of Function Definitions################ Main Script ################################### Get name of User Account to check#echo \"Step #1 - Determine User Account name to delete \"echoLINE1=\"Please enter the username of the user\"LINE2=\"account you wish to delete from system:\"get_answerUSER_ACCOUNT=$ANSWER## Double check with script user that this is the correct User Account#LINE1=\"Is $USER_ACCOUNT the user account\"LINE2=\"you wish to delete from the system?[ y/n ]:\"get_answer############################################################### Check that USER_ACCOUNT is really an account on the system#USER_ACCOUNT_RECORD=$(cat /etc/passwd | grep -w $USER_ACCOUNT)#if [ $? -eq 1 ] # If the account is not found, exit scriptthen echo echo \"Account, $USER_ACCOUNT, not found.\" echo \"Leaving the script...\" echo exitfi#echoecho \"I found this record:\"echo $USER_ACCOUNT_RECORDecho#LINE1=\"Is this the correct User Account?[y/n]:\"get_answer### Call process_answer function:# if user answers anything but \"yes\", exit script#EXIT_LINE1=\"Because the account, $USER_ACCOUNT, is not \"EXIT_LINE2=\"the one you wish to delete, we are leaving the script...\"process_anser################################################################# Search for any running processes that belong to the User Account#echoecho \"Step #2 - Find process on system belonging to user account\"echoecho \"$USER_ACCOUNT has the following processes running: \"echo#ps -u $USER_ACCOUNT #List the processes running#case $? in1) # No processes running for this User Account # echo \"There are no processes for this account currently running.\" echo;;0) # Processes running for this User Account. # Ask Script User if wants us to kill the processes. # unset ANSWER # I think this line is not needed LINE1=\"Would you like me to kill the process(es)? [y/n]:\" get_answer # case $ANSWER in y|Y|YES|yes|Yes|yEs|yeS|YEs|yES) # if user answer \"yes\", #kill User Account processes # echo # # Clean-up temp file upon signals # trap \"rm $USER_ACCOUNT_Running_Process.rpt\" SIGTERM SIGINT SIGQUIT # # List user processes running ps -u $USER_ACCOUNT &gt; $USER_ACCOUNT_Running_Process.rpt # exec &lt; $USER_ACCOUNT_Running_Process.rpt # Make report Std Input # read USER_PROCESS_REC # First record will be blank read USER_PROCESS_REC # while [ $? -eq 0 ] do # obtain PID USER_PID=$(echo $USER_PROCESS_REC | cut -d \" \" -f1 ) kill -9 $USER_PID echo \"Killed process $USER_PID\" read USER_PROCESS_REC done # echo # rm $USER_ACCOUNT_Running_Process.rpt # Remove temp report ;; *) # If user answers anything but \"yes\", do not kill. echo echo \"Will not kill the process(es).\" echo ;; esac;;esac##################################################################################### Create a report of all files owned by User Account#echoecho \"Step #3 - Find files on system belonging to user account\"echoecho \"Creating a report of all files owned by $USER_ACCOUNT.\"echoecho \"It is recommended that you backup/archive these files.\"echo \"and then do one of two things:\"echo \" 1) Delete the files\"echo \" 2) Change the files' ownership to a current user account.\"echoecho \"Please wait. This may take a while...\"#REPORT_DATE=`date +%y%m%d`REPORT_FILE=$USER_ACCOUNT\"_Files_\"$REPORT_DATE\".rpt\"#find / -user $USER_ACCOUNT &gt; $REPORT_FILE 2&gt;/dev/null#echoecho \"Report is complete.\"echo \"Name of report: $REPORT_FILE\"echo \"Location of report: `pwd`\"echo################################################################## Remove User Accountechoecho \"Step #4 - Remove user account\"echo#LINE1=\"Do you wish to remove $USER_ACCOUNT's account from system? [y/n]:\"get_answer## Cass process_answer function:# if user answers anything but \"yes\", exit script#EXIT_LINE1=\"Since you do not wish to remove the user account.\"EXIT_LINE2=\"$USER_ACCOUNT at this time, exiting the script...\"process_answer#userdel $USER_ACCOUNT # delete user accountechoecho \"User account, $USER_ACCOUNT, has been removed\"echo#","categories":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}],"tags":[{"name":"Shell Linux 运维","slug":"Shell-Linux-运维","permalink":"http://LTSpider.github.io/tags/Shell-Linux-运维/"}],"keywords":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}]},{"title":"Shell命令运算","slug":"shell相关命令运算","date":"2019-07-26T04:17:52.652Z","updated":"2019-07-29T08:23:18.987Z","comments":true,"path":"2019/07/26/shell相关命令运算/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/shell相关命令运算/","excerpt":"","text":"exit 命令12345678#!/bin/bash#退出状态码，最大为255，超过则进行模运算#testing the exit statusvar1=10var2=20var3=$[ $var1 + $var2]echo The answer is $var3exit 5 使用expr执行数学运算1234567#!/bin/bash#An example of using the expr commandvar1=10var2=20var3=`expr $var2 / $var1`echo \"The result is $var3\" 使用内联重定向计算表达式123456789101112131415#!/bin/bashvar1=10.45var2=43.67var3=33.2var4=71var5=`bc &lt;&lt;EOFscale=4a1 = $var1 * $var2b1 = $var3 * $var4a1 + b1EOF`echo The final answer for this mess is $var5 使用方括号执行数学运算1234567#!/bin/bashvar1=10var2=50var3=45var4=$[$var1 * ($var2 - $var3)]echo 'The final result is '$var4 使用自定义变量123456789#!/bin/bash#testing variablesdays=10guest=\"Katie\"echo \"$guest logged in $days days age\"guest=\"Katie2\"days=5echo \"$guest logged in $days days age\" 反引号的使用12345#!/bin/bash#using the backtick character 会把反引号里面当作一条命令来执行testing=`date`echo \"The date and time are:$testing\" 在脚本中使用bc12345#!/bin/bashvar1=100var2=45var3=`echo \"scale=4; $var1 / $var2\" | bc`echo The answer for this is $var3 显示时间和登录者123456789#!/bin/bash#This script displays the date and who's logged on#如果想在同一行显示#echo -n -e 'The time is:\\n\\n'echo The time is:dateecho The one who has been logged is:who 显示系统变量和转义字符123456789#!/bin/bash#display user information from systemecho \"User info fro userId:$USER\"echo UID:$UIDecho HOME:$HOME#换行echo -e '\\n' echo 'The cost of the item is \\$15' 通过反引号获得当前日期并生成唯一文件名12345#!/bin/bash#copy the /usr/bin directory listing to a log filetoday=`date +%y%m%d`ls /usr/bin -al &gt; log.$today","categories":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}],"tags":[{"name":"Shell Linux 运维","slug":"Shell-Linux-运维","permalink":"http://LTSpider.github.io/tags/Shell-Linux-运维/"}],"keywords":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}]},{"title":"CentOs7 下的Python3 的安装","slug":"CentOs7下Python3-相关安装","date":"2019-07-26T04:17:52.645Z","updated":"2019-07-29T08:24:01.630Z","comments":true,"path":"2019/07/26/CentOs7下Python3-相关安装/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/CentOs7下Python3-相关安装/","excerpt":"","text":"Centos7下python3的安装，virtualenv、virtualenvwrapper虚拟独立环境管理工具 安装准备工具: # yum groupinstall “Developmenttools” # yum install zlib-devel bzip2-developenssl-devel ncurses-devel sqlite-devel readline-develtk-devel gdbm-develdb4-devel libpcap-devel xz-devel ————————————-安装python————————————— 下载python3源码 wget wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tar.xz 解压：Python-3.6.5.tar.xz 解压后新建文件夹将Python-3.6.5放在/usr/local/python3下 1.# mkdir python3 2.# mv Python-3.6.5 python3 3.# cd python3/Python-3.6.5 执行如下代码安装 ···························**–prefix选项是配置安装的路径,执行后所有资源文件放在/usr/local/python3.6**的路径中 # ./configure –prefix=/usr/local/python3 –enable-optimizations # make # make install 终于可以修改软链接了 [plain] view plain copy # ln -s /usr/local/python3/bin/python3 /usr/bin/python3 # ln -s /usr/local/python3/bin/pip3.6 /usr/bin/pip3 ————————————-安装-virtualenv————————————– 使用编译安装python3过程中的pip包管理工具，安装virtualenv python环境隔离工具 pip3 installvirtualenv 在~/.bashrc文件中加上： export PIP_REQUIRE_VIRTUALENV=true 或者让在执行pip的时候让系统自动开启虚拟环境： export PIP_RESPECT_VIRTUALENV=true source ~/.bashrc使其生效 建立python3独立环境 virtualenv -p/usr/local/bin/python3 /py3env 使用python3环境 ./py3env/bin/activate 安装Virtaulenvwrapper是virtualenv的扩展包，用于更方便管理虚拟环境 easy_installvirtualenvwrapper(这个过程有点慢，莫急) 此时还不能使用virtualenvwrapper，默认virtualenvwrapper安装在/usr/bin下面，实际上你需要运行virtualenvwrapper.sh文件才行，配置环境 打开~/.bashrc最终配置环境如下说明： exportPIP_REQUIRE_VIRTUALENV=true exportPIP_RESPECT_VIRTUALENV=true exportWORKON_HOME=$HOME/.virtualenvs source/usr/bin/virtualenvwrapper.sh exportVIRTUALENVWRAPPER_PYTHON=/usr/bin/python2.7 #防止环境变量$PATH中已有其它环境的python 配置好后，source一下使其生效 使用virtualenvwrapper工具管理隔离环境命令如下： 列出虚拟环境列表 workon 也可以使用 lsvirtualenv 新建虚拟环境 mkvirtualenv [虚拟环境名称] 启动/切换虚拟环境 workon [虚拟环境名称] 删除虚拟环境 rmvirtualenv [虚拟环境名称] 离开虚拟环境 deactivate 查看当前虚拟环境所安装的包: 命令:pip list","categories":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://LTSpider.github.io/tags/Linux/"}],"keywords":[{"name":"ops","slug":"ops","permalink":"http://LTSpider.github.io/categories/ops/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-07-26T04:03:14.447Z","updated":"2019-06-04T07:58:22.994Z","comments":true,"path":"2019/07/26/hello-world/","link":"","permalink":"http://LTSpider.github.io/2019/07/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[],"keywords":[]}]}